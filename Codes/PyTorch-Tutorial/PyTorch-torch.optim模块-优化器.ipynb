{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch-torch.optim模块-优化器.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOFRaKKyYuGoKMQ6rGamN5i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veager/StudyNotes/blob/new/Codes/PyTorch-Tutorial/PyTorch-torch.optim%E6%A8%A1%E5%9D%97-%E4%BC%98%E5%8C%96%E5%99%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch `torch.optim` 模块 优化器\n",
        "\n",
        "参考资料：\n",
        "\n",
        "- 博客：Pytorch torch.optim 模块 优化器, [site](https://www.cnblogs.com/veager/articles/16305151.html)\n",
        "\n",
        "- Github：Codes/PyTorch-Tutorial/PyTorch-torch.optim模块-优化器.ipynb"
      ],
      "metadata": {
        "id": "NJwa_bZbv-lJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0 加载数据"
      ],
      "metadata": {
        "id": "UxdXJYwHIDML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "84wmVTGLFS7E"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.1 加载 Iris 数据集"
      ],
      "metadata": {
        "id": "C7i7XtmoISUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "data = load_diabetes()\n",
        "X = data.data\n",
        "Y = data.target\n",
        "\n",
        "# 将输入输出数据归一化到 [0, 1] 之间\n",
        "scaler_X = MinMaxScaler().fit(X)\n",
        "scaler_Y = MinMaxScaler().fit(np.expand_dims(Y, axis=1))\n",
        "\n",
        "Xs = scaler_X.transform(X)\n",
        "Ys = scaler_Y.transform(np.expand_dims(Y, axis=1))\n",
        "\n",
        "print(Xs.shape, Ys.shape)\n",
        "print(Xs[:5], Ys[:5])\n",
        "print(pd.DataFrame(Xs).describe(), pd.DataFrame(Ys).describe())\n",
        "\n",
        "# 将数据转换为 tensor 类型 \n",
        "Xs_tensor = torch.tensor(Xs, dtype=torch.float)\n",
        "Ys_tensor = torch.tensor(Ys, dtype=torch.float)"
      ],
      "metadata": {
        "id": "ugyTu1xpZ4vG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0bac995-eb23-4ed9-a36f-699c7df9755e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(442, 10) (442, 1)\n",
            "[[0.66666667 1.         0.58264463 0.54929577 0.29411765 0.25697211\n",
            "  0.20779221 0.28208745 0.56221737 0.43939394]\n",
            " [0.48333333 0.         0.14876033 0.35211268 0.42156863 0.30677291\n",
            "  0.62337662 0.14104372 0.22244301 0.16666667]\n",
            " [0.88333333 1.         0.51652893 0.43661972 0.28921569 0.25896414\n",
            "  0.24675325 0.28208745 0.49658437 0.40909091]\n",
            " [0.08333333 0.         0.30165289 0.30985915 0.49509804 0.44721116\n",
            "  0.23376623 0.42313117 0.57293604 0.46969697]\n",
            " [0.51666667 0.         0.20661157 0.54929577 0.46568627 0.41733068\n",
            "  0.38961039 0.28208745 0.36236911 0.33333333]] [[0.39252336]\n",
            " [0.15576324]\n",
            " [0.36137072]\n",
            " [0.56386293]\n",
            " [0.34267913]]\n",
            "                0           1           2           3           4           5  \\\n",
            "count  442.000000  442.000000  442.000000  442.000000  442.000000  442.000000   \n",
            "mean     0.491968    0.468326    0.346107    0.459818    0.451668    0.367725   \n",
            "std      0.218484    0.499561    0.182567    0.194806    0.169647    0.151460   \n",
            "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
            "25%      0.320833    0.000000    0.214876    0.309859    0.329657    0.271165   \n",
            "50%      0.516667    0.000000    0.318182    0.436620    0.436275    0.355578   \n",
            "75%      0.666667    1.000000    0.465909    0.605634    0.552696    0.462649   \n",
            "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
            "\n",
            "                6           7           8           9  \n",
            "count  442.000000  442.000000  442.000000  442.000000  \n",
            "mean     0.360889    0.291996    0.485557    0.503942  \n",
            "std      0.167977    0.182010    0.183364    0.174187  \n",
            "min      0.000000    0.000000    0.000000    0.000000  \n",
            "25%      0.237013    0.141044    0.357528    0.382576  \n",
            "50%      0.337662    0.282087    0.478057    0.500000  \n",
            "75%      0.464286    0.423131    0.610446    0.606061  \n",
            "max      1.000000    1.000000    1.000000    1.000000                   0\n",
            "count  442.000000\n",
            "mean     0.396054\n",
            "std      0.240165\n",
            "min      0.000000\n",
            "25%      0.193146\n",
            "50%      0.359813\n",
            "75%      0.580997\n",
            "max      1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.2 全局参数设置"
      ],
      "metadata": {
        "id": "Xf0kFGsGKWna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 样本信息，划分的数据集\n",
        "N_SAMPLE = Xs_tensor.size()[0]              # 样本总数\n",
        "N_TRAIN = int(N_SAMPLE * 0.7)               # 训练样本数\n",
        "N_VALID = int(N_SAMPLE * 0.2)               # 验证样本数\n",
        "N_TEST = N_SAMPLE - N_TRAIN - N_VALID       # 测试样本数\n",
        "\n",
        "\n",
        "# 训练过程超参数设置\n",
        "BATCH_SIZE = 64\n",
        "N_EPOCH = 1000\n",
        "LEARNING_RATE = 0.05\n",
        "\n",
        "\n",
        "# 神经网络模型参数\n",
        "HIDDEN_DIM = 4\n",
        "INPUT_DIM = Xs_tensor.size()[1]     # sizes of input data and output data\n",
        "OUTPUT_DIM = Ys_tensor.size()[1]\n",
        "print(\"NN Structure:\", INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
        "\n",
        "\n",
        "# 设置 device，如果 GPU 可用，则使用\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"device:\", DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5U3xRjEKVxF",
        "outputId": "d7878a6f-406c-4fa6-8079-05dc3a89c31e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN Structure: 10 4 1\n",
            "device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.3 创建结构化数据"
      ],
      "metadata": {
        "id": "FaMqo_x-Ioej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, Dataset, random_split, DataLoader\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "\n",
        "# DataSet 类\n",
        "ds = TensorDataset(Xs_tensor, Ys_tensor)\n",
        "\n",
        "# split training, validation, testing data\n",
        "ds_train, ds_vaild, ds_test = random_split(ds, lengths=[N_TRAIN, N_VALID, N_TEST])\n",
        "print(len(ds_train), len(ds_vaild), len(ds_test))\n",
        "\n",
        "\n",
        "# DataLoader of train data, valid data, test data\n",
        "dl_train = DataLoader(ds_train, batch_size = BATCH_SIZE, shuffle = True,\n",
        "    collate_fn = lambda x: tuple(x_.to(DEVICE) for x_ in default_collate(x))\n",
        "    )\n",
        "\n",
        "dl_valid = DataLoader(ds_vaild, batch_size = BATCH_SIZE, # default shuffle = False,\n",
        "    collate_fn = lambda x: tuple(x_.to(DEVICE) for x_ in default_collate(x))\n",
        "    )\n",
        "\n",
        "dl_test = DataLoader(ds_test, batch_size = BATCH_SIZE, # default shuffle = False,\n",
        "    collate_fn = lambda x: tuple(x_.to(DEVICE) for x_ in default_collate(x))\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ngWqkICIYbz",
        "outputId": "11f11fe0-6234-4dea-d634-16d127a3a380"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "309 88 45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.4 定义模型"
      ],
      "metadata": {
        "id": "z_BqissYIaGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 方式 2: 全部使用 层类（nn.Module 类）\n",
        "class BPNNModeler2(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "\n",
        "        super(BPNNModeler2, self).__init__()\n",
        "\n",
        "        # Layer 1\n",
        "        self.layer1_linear = nn.Linear(input_dim, hidden_dim)\n",
        "        self.layer1_sigmoid = nn.Sigmoid()\n",
        "        # Layer 2\n",
        "        self.layer2_linear = nn.Linear(hidden_dim, output_dim)\n",
        "        self.layer2_sigmoid = nn.Sigmoid()\n",
        "        # Output\n",
        "        self.layer2_flattern = nn.Flatten(0, -1)\n",
        "    \n",
        "    # 正向传播\n",
        "    def forward(self, x):\n",
        "        # Layer 1\n",
        "        out_layer1 = self.layer1_linear(x)\n",
        "        out_layer1 = self.layer1_sigmoid(out_layer1)\n",
        "        # Layer 2\n",
        "        out_layer2 = self.layer2_linear(out_layer1)\n",
        "        out_layer2 = self.layer2_sigmoid(out_layer2)\n",
        "        # Output\n",
        "        out = self.layer2_flattern(out_layer2)\n",
        "        return out\n",
        "# =============================================================================="
      ],
      "metadata": {
        "id": "3AiyB78lIXa2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.4.1 初始化模型"
      ],
      "metadata": {
        "id": "qfQ_PEy3LYoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义模型\n",
        "model = BPNNModeler2(input_dim = INPUT_DIM, hidden_dim = HIDDEN_DIM, output_dim = OUTPUT_DIM)\n",
        "model = model.to(DEVICE)"
      ],
      "metadata": {
        "id": "0xniWxzQKikG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.5 定义损失函数"
      ],
      "metadata": {
        "id": "kOBsRGv-I2wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义 损失函数 MSE 损失\n",
        "def loss_func(model_out, target, reduction='mean'):\n",
        "    loss = F.mse_loss(model_out, target, reduction=reduction) \n",
        "    return loss"
      ],
      "metadata": {
        "id": "_mf3sHJ3I1C7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 优化器：`torch.optim` 模块"
      ],
      "metadata": {
        "id": "M9Ff4jM2wptd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 优化器 `torch.optim.Optimizer` 类"
      ],
      "metadata": {
        "id": "RdyNmx_SLdMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.1 主要参数"
      ],
      "metadata": {
        "id": "kkm-7LLELrPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**实例**：传入参数组"
      ],
      "metadata": {
        "id": "yaKlwwNCLvgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_1 = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "optimizer_2 = torch.optim.SGD(\n",
        "    # 参数组 1：模型的第1个线性层。特别设置参数 lr\n",
        "    [{'params': model.layer1_linear.parameters(), 'lr': 0.5},  \n",
        "    # 参数组 2：模型的的第2个线性层。未设置参数，由全局参数决定\n",
        "     {'params': model.layer2_linear.parameters()}], \n",
        "    # 在 list 外的，为全局参数\n",
        "    lr = LEARNING_RATE)\n",
        "\n",
        "# 输出优化器的状态和传入参数\n",
        "print(optimizer_2.state_dict())\n",
        "# Output\n",
        "# {'state': {}, 'param_groups': [{'lr': 0.5, ...}, \n",
        "#                                {'lr': 0.1, ...}]}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkePMt1eLu4D",
        "outputId": "ccdda4e4-db2f-444b-c0c8-3427aee435f5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'state': {}, 'param_groups': [{'lr': 0.5, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'params': [0, 1]}, {'lr': 0.05, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'params': [2, 3]}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.2 主要方法"
      ],
      "metadata": {
        "id": "x-o5HOBYMD1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 输出 优化器的状态和 传入参数\n",
        "print(optimizer_1.state_dict())\n",
        "print(optimizer_2.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChNlfyk4MFgb",
        "outputId": "24bdd827-514e-44ca-93a1-dd4884f700ea"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'state': {}, 'param_groups': [{'lr': 0.05, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'params': [0, 1, 2, 3]}]}\n",
            "{'state': {}, 'param_groups': [{'lr': 0.5, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'params': [0, 1]}, {'lr': 0.05, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'params': [2, 3]}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.3 主要属性"
      ],
      "metadata": {
        "id": "7HWMBcUoMI1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 优化器 默认参数\n",
        "print(optimizer_2.defaults)\n",
        "\n",
        "# 优化器 参数组 具体信息\n",
        "print(optimizer_2.param_groups)\n",
        "print(optimizer_2.state_dict()['param_groups'])\n",
        "\n",
        "# 获取 参数组1 的参数， 返回为 list 类型\n",
        "param_group_1 = optimizer_2.param_groups[0]['params']\n",
        "print(type(param_group_1), type(param_group_1[0]))\n",
        "\n",
        "# 获取 参数组1 的 第1个参数\n",
        "print(param_group_1[0].data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pYwkrBwMLU3",
        "outputId": "276fc7cb-d9d2-4705-f77d-ef029a63e66a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'lr': 0.05, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False}\n",
            "[{'params': [Parameter containing:\n",
            "tensor([[ 0.0235,  0.2899,  0.0748, -0.0052, -0.0145,  0.0643,  0.0654,  0.0825,\n",
            "         -0.2345,  0.2105],\n",
            "        [-0.1453, -0.1634, -0.2978, -0.0543, -0.2249,  0.1285,  0.0708, -0.0051,\n",
            "         -0.1920,  0.2287],\n",
            "        [ 0.2997, -0.2347,  0.2108,  0.2138, -0.0253,  0.3065, -0.2477, -0.1689,\n",
            "          0.3011,  0.2740],\n",
            "        [ 0.1893, -0.0620, -0.0113,  0.1808, -0.0836, -0.0066, -0.2588, -0.1311,\n",
            "         -0.1181,  0.1099]], requires_grad=True), Parameter containing:\n",
            "tensor([-0.2799, -0.1103, -0.0127, -0.0850], requires_grad=True)], 'lr': 0.5, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False}, {'params': [Parameter containing:\n",
            "tensor([[ 0.4514, -0.3771,  0.2805,  0.4148]], requires_grad=True), Parameter containing:\n",
            "tensor([0.4143], requires_grad=True)], 'lr': 0.05, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False}]\n",
            "[{'lr': 0.5, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'params': [0, 1]}, {'lr': 0.05, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'params': [2, 3]}]\n",
            "<class 'list'> <class 'torch.nn.parameter.Parameter'>\n",
            "tensor([[ 0.0235,  0.2899,  0.0748, -0.0052, -0.0145,  0.0643,  0.0654,  0.0825,\n",
            "         -0.2345,  0.2105],\n",
            "        [-0.1453, -0.1634, -0.2978, -0.0543, -0.2249,  0.1285,  0.0708, -0.0051,\n",
            "         -0.1920,  0.2287],\n",
            "        [ 0.2997, -0.2347,  0.2108,  0.2138, -0.0253,  0.3065, -0.2477, -0.1689,\n",
            "          0.3011,  0.2740],\n",
            "        [ 0.1893, -0.0620, -0.0113,  0.1808, -0.0836, -0.0066, -0.2588, -0.1311,\n",
            "         -0.1181,  0.1099]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 优化实例"
      ],
      "metadata": {
        "id": "ltvvEdRTMVQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.1 优化器和模型的`.zero_grad()`方法使用区别"
      ],
      "metadata": {
        "id": "0JDYE_62MWVb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**实例 1**： 网络模型训练实例，使用 `optimizer.zero_grad()` 方法\n",
        "\n"
      ],
      "metadata": {
        "id": "GDv5KamfMhGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: 定义优化器\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "for epoch in range(N_EPOCH):\n",
        "\n",
        "    train_total_loss = 0.\n",
        "\n",
        "    for i, (X_batch, Y_batch) in enumerate(dl_train): \n",
        "\n",
        "        optimizer.zero_grad()    # Step 2: 参数梯度归零\n",
        "\n",
        "        out = model(X_batch)                           \n",
        "        loss = loss_func(out, Y_batch.flatten())  \n",
        "\n",
        "        loss.backward()          # Step 3: 反向传播，计算梯度\n",
        "        optimizer.step()         # Step 4: 执行一步优化，更新参数\n",
        "\n",
        "        train_total_loss += loss.item()"
      ],
      "metadata": {
        "id": "VZVtlYBGMUhX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**实例 2**：使用梯度下降更新参数，使用 `model.zero_grad()` 方法"
      ],
      "metadata": {
        "id": "ZTBYGOv8MsTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(N_EPOCH):\n",
        "    \n",
        "    train_total_loss = 0.\n",
        "\n",
        "    for i, (X_batch, Y_batch) in enumerate(dl_train):\n",
        "\n",
        "        model.zero_grad()       # Step 1: 参数梯度归零\n",
        "        out = model(X_batch)      \n",
        "\n",
        "        loss = loss_func(out, Y_batch.flatten())  \n",
        "        loss.backward()         # Step 2: 反向传播，计算梯度\n",
        "\n",
        "        with torch.no_grad():   \n",
        "            for param in model.parameters():\n",
        "                param -= LEARNING_RATE * param.grad     # Step 3: 更新参数\n",
        "        \n",
        "        train_total_loss += loss.item()"
      ],
      "metadata": {
        "id": "YFZdkRZVMWJS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.2 部分参数训练"
      ],
      "metadata": {
        "id": "bnKaOpycM59B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LEANING_RATE = 0.1\n",
        "N_EPOCH = 10\n",
        "\n",
        "# 模型初始参数\n",
        "w1 = model.layer1_linear.weight.data.clone()\n",
        "b1 = model.layer1_linear.bias.data.clone()\n",
        "w2 = model.layer2_linear.weight.data.clone()\n",
        "b2 = model.layer2_linear.bias.data.clone()\n",
        "\n",
        "# 仅训练 model2 第2层的线性层\n",
        "param = model.layer2_linear.parameters()\n",
        "\n",
        "# Step 1: 定义优化器\n",
        "optimizer = torch.optim.SGD(param, lr=LEANING_RATE)\n",
        "\n",
        "for epoch in range(N_EPOCH):\n",
        "    train_total_loss = 0.\n",
        "    for i, (X_batch, Y_batch) in enumerate(dl_train): \n",
        "\n",
        "        optimizer.zero_grad()    # Step 2: 参数梯度归零\n",
        "\n",
        "        out = model(X_batch)                           \n",
        "        loss = loss_func(out, Y_batch.flatten())  \n",
        "\n",
        "        loss.backward()          # Step 3: 反向传播，计算梯度\n",
        "        optimizer.step()         # Step 4: 执行一步优化，更新参数\n",
        "\n",
        "        train_total_loss += loss.item()\n",
        "    \n",
        "    # 检查 参数 是否改变\n",
        "    w1_new = model.layer1_linear.weight.data\n",
        "    b1_new = model.layer1_linear.bias.data\n",
        "    w2_new = model.layer2_linear.weight.data\n",
        "    b2_new = model.layer2_linear.bias.data\n",
        "    print(epoch + 1, \n",
        "          'w1:', torch.sum(torch.abs(w1 - w1_new)).item(),\n",
        "          'b1:', torch.sum(torch.abs(b1 - b1_new)).item(),\n",
        "          'w2:', torch.sum(torch.abs(w2 - w2_new)).item(),\n",
        "          'b2:', torch.sum(torch.abs(b2 - b2_new)).item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5na_g1MdM7lh",
        "outputId": "0f1e206b-a39d-47b8-c3c6-1a55144ff7c7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 w1: 0.0 b1: 0.0 w2: 0.0031652748584747314 b2: 0.0001932978630065918\n",
            "2 w1: 0.0 b1: 0.0 w2: 0.0065653324127197266 b2: 0.0001818239688873291\n",
            "3 w1: 0.0 b1: 0.0 w2: 0.00960451364517212 b2: 0.0004006922245025635\n",
            "4 w1: 0.0 b1: 0.0 w2: 0.012634485960006714 b2: 0.0006747841835021973\n",
            "5 w1: 0.0 b1: 0.0 w2: 0.015293598175048828 b2: 0.0012349188327789307\n",
            "6 w1: 0.0 b1: 0.0 w2: 0.01871246099472046 b2: 0.0012495815753936768\n",
            "7 w1: 0.0 b1: 0.0 w2: 0.022029966115951538 b2: 0.0013096928596496582\n",
            "8 w1: 0.0 b1: 0.0 w2: 0.02543380856513977 b2: 0.0013377070426940918\n",
            "9 w1: 0.0 b1: 0.0 w2: 0.028123706579208374 b2: 0.0018067657947540283\n",
            "10 w1: 0.0 b1: 0.0 w2: 0.03120484948158264 b2: 0.0019614696502685547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 学习速度策略：`torch.optim.lr_scheduler` 模块"
      ],
      "metadata": {
        "id": "FL_LGbNBJrM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(\n",
        "    [{'params': model.parameters(), 'initial_lr': LEARNING_RATE}],  # 需要向 param group 传入 initial_lr 参数\n",
        "    lr = LEARNING_RATE)     # lr 参数不可忽略 "
      ],
      "metadata": {
        "id": "tALw4bp5Jx3Z"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 `StepLR()`"
      ],
      "metadata": {
        "id": "YHItKbD0J7ig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCH = 50\n",
        "LEARNING_RATE = 1.\n",
        "\n",
        "# 定义 优化器\n",
        "optimizer = torch.optim.SGD(\n",
        "    [{'params': model.parameters(), 'initial_lr': LEARNING_RATE}],\n",
        "    lr = LEARNING_RATE)  # lr 参数不可忽略\n",
        "\n",
        "# 定义 学习速率调整方式\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "for epoch in range(N_EPOCH):\n",
        "    train_total_loss = 0.\n",
        "    \n",
        "    # 训练\n",
        "    for i, (X_batch, Y_batch) in enumerate(dl_train):  \n",
        "        optimizer.zero_grad()\n",
        "        out = model(X_batch)                        \n",
        "        loss = loss_func(out, Y_batch.flatten())    \n",
        "        loss.backward()\n",
        "        optimizer.step()   \n",
        "        train_total_loss += loss.item() \n",
        "\n",
        "    # 更新 学习速率\n",
        "    scheduler.step() \n",
        "\n",
        "    # Print Traing information\n",
        "    print('Epoch: {0:>4}, Train Loss: {1:>10.5f}, LR: {2:>10.8f}, Init LR: {3:>10.8f}'.format(\n",
        "        epoch+1, \n",
        "        train_total_loss, \n",
        "        optimizer.param_groups[0]['lr'], \n",
        "        optimizer.param_groups[0]['initial_lr']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBc9f11kJ5We",
        "outputId": "6e293edf-405c-4c0b-aa09-482119a99781"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:    1, Train Loss:    0.15933, LR: 1.00000000, Init LR: 1.00000000\n",
            "Epoch:    2, Train Loss:    0.15805, LR: 1.00000000, Init LR: 1.00000000\n",
            "Epoch:    3, Train Loss:    0.15863, LR: 1.00000000, Init LR: 1.00000000\n",
            "Epoch:    4, Train Loss:    0.15653, LR: 1.00000000, Init LR: 1.00000000\n",
            "Epoch:    5, Train Loss:    0.15495, LR: 1.00000000, Init LR: 1.00000000\n",
            "Epoch:    6, Train Loss:    0.15516, LR: 1.00000000, Init LR: 1.00000000\n",
            "Epoch:    7, Train Loss:    0.15625, LR: 1.00000000, Init LR: 1.00000000\n",
            "Epoch:    8, Train Loss:    0.15471, LR: 1.00000000, Init LR: 1.00000000\n",
            "Epoch:    9, Train Loss:    0.15203, LR: 1.00000000, Init LR: 1.00000000\n",
            "Epoch:   10, Train Loss:    0.15387, LR: 0.10000000, Init LR: 1.00000000\n",
            "Epoch:   11, Train Loss:    0.15218, LR: 0.10000000, Init LR: 1.00000000\n",
            "Epoch:   12, Train Loss:    0.15082, LR: 0.10000000, Init LR: 1.00000000\n",
            "Epoch:   13, Train Loss:    0.15110, LR: 0.10000000, Init LR: 1.00000000\n",
            "Epoch:   14, Train Loss:    0.15186, LR: 0.10000000, Init LR: 1.00000000\n",
            "Epoch:   15, Train Loss:    0.15239, LR: 0.10000000, Init LR: 1.00000000\n",
            "Epoch:   16, Train Loss:    0.15302, LR: 0.10000000, Init LR: 1.00000000\n",
            "Epoch:   17, Train Loss:    0.15055, LR: 0.10000000, Init LR: 1.00000000\n",
            "Epoch:   18, Train Loss:    0.15192, LR: 0.10000000, Init LR: 1.00000000\n",
            "Epoch:   19, Train Loss:    0.15205, LR: 0.10000000, Init LR: 1.00000000\n",
            "Epoch:   20, Train Loss:    0.15335, LR: 0.01000000, Init LR: 1.00000000\n",
            "Epoch:   21, Train Loss:    0.15114, LR: 0.01000000, Init LR: 1.00000000\n",
            "Epoch:   22, Train Loss:    0.15060, LR: 0.01000000, Init LR: 1.00000000\n",
            "Epoch:   23, Train Loss:    0.15230, LR: 0.01000000, Init LR: 1.00000000\n",
            "Epoch:   24, Train Loss:    0.15099, LR: 0.01000000, Init LR: 1.00000000\n",
            "Epoch:   25, Train Loss:    0.15044, LR: 0.01000000, Init LR: 1.00000000\n",
            "Epoch:   26, Train Loss:    0.15310, LR: 0.01000000, Init LR: 1.00000000\n",
            "Epoch:   27, Train Loss:    0.15125, LR: 0.01000000, Init LR: 1.00000000\n",
            "Epoch:   28, Train Loss:    0.15200, LR: 0.01000000, Init LR: 1.00000000\n",
            "Epoch:   29, Train Loss:    0.15198, LR: 0.01000000, Init LR: 1.00000000\n",
            "Epoch:   30, Train Loss:    0.15170, LR: 0.00100000, Init LR: 1.00000000\n",
            "Epoch:   31, Train Loss:    0.15124, LR: 0.00100000, Init LR: 1.00000000\n",
            "Epoch:   32, Train Loss:    0.15095, LR: 0.00100000, Init LR: 1.00000000\n",
            "Epoch:   33, Train Loss:    0.15095, LR: 0.00100000, Init LR: 1.00000000\n",
            "Epoch:   34, Train Loss:    0.15044, LR: 0.00100000, Init LR: 1.00000000\n",
            "Epoch:   35, Train Loss:    0.15330, LR: 0.00100000, Init LR: 1.00000000\n",
            "Epoch:   36, Train Loss:    0.15110, LR: 0.00100000, Init LR: 1.00000000\n",
            "Epoch:   37, Train Loss:    0.15085, LR: 0.00100000, Init LR: 1.00000000\n",
            "Epoch:   38, Train Loss:    0.15059, LR: 0.00100000, Init LR: 1.00000000\n",
            "Epoch:   39, Train Loss:    0.15196, LR: 0.00100000, Init LR: 1.00000000\n",
            "Epoch:   40, Train Loss:    0.15270, LR: 0.00010000, Init LR: 1.00000000\n",
            "Epoch:   41, Train Loss:    0.15071, LR: 0.00010000, Init LR: 1.00000000\n",
            "Epoch:   42, Train Loss:    0.15201, LR: 0.00010000, Init LR: 1.00000000\n",
            "Epoch:   43, Train Loss:    0.15223, LR: 0.00010000, Init LR: 1.00000000\n",
            "Epoch:   44, Train Loss:    0.15049, LR: 0.00010000, Init LR: 1.00000000\n",
            "Epoch:   45, Train Loss:    0.15155, LR: 0.00010000, Init LR: 1.00000000\n",
            "Epoch:   46, Train Loss:    0.15194, LR: 0.00010000, Init LR: 1.00000000\n",
            "Epoch:   47, Train Loss:    0.15121, LR: 0.00010000, Init LR: 1.00000000\n",
            "Epoch:   48, Train Loss:    0.15246, LR: 0.00010000, Init LR: 1.00000000\n",
            "Epoch:   49, Train Loss:    0.15266, LR: 0.00010000, Init LR: 1.00000000\n",
            "Epoch:   50, Train Loss:    0.15197, LR: 0.00001000, Init LR: 1.00000000\n"
          ]
        }
      ]
    }
  ]
}