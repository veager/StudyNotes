{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch-torch.nn.Module类-构建模型.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMjgir/KqdpE7uJ1s88xKAs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veager/StudyNotes/blob/new/Codes/PyTorch-Tutorial/PyTorch-torch.nn.Module%E7%B1%BB-%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch torch.nn.Module 类 构建模型\n",
        "\n",
        "参考资料：\n",
        "\n",
        "- 博客：PyTorch torch.nn.Module 类 构建模型，[地址](https://www.cnblogs.com/veager/articles/16305187.html)\n",
        "\n",
        "- GitHub：Codes/PyTorch-Tutorial/PyTorch-torch.nn.Module类-构建模型.ipynb"
      ],
      "metadata": {
        "id": "gwaGEdPqEy-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0 加载数据"
      ],
      "metadata": {
        "id": "UxdXJYwHIDML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "84wmVTGLFS7E"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.1 加载 Iris 数据集"
      ],
      "metadata": {
        "id": "C7i7XtmoISUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "data = load_diabetes()\n",
        "X = data.data\n",
        "Y = data.target\n",
        "\n",
        "# 将输入输出数据归一化到 [0, 1] 之间\n",
        "scaler_X = MinMaxScaler().fit(X)\n",
        "scaler_Y = MinMaxScaler().fit(np.expand_dims(Y, axis=1))\n",
        "\n",
        "Xs = scaler_X.transform(X)\n",
        "Ys = scaler_Y.transform(np.expand_dims(Y, axis=1))\n",
        "\n",
        "print(Xs.shape, Ys.shape)\n",
        "print(Xs[:5], Ys[:5])\n",
        "print(pd.DataFrame(Xs).describe(), pd.DataFrame(Ys).describe())\n",
        "\n",
        "# 将数据转换为 tensor 类型 \n",
        "Xs_tensor = torch.tensor(Xs, dtype=torch.float)\n",
        "Ys_tensor = torch.tensor(Ys, dtype=torch.float)"
      ],
      "metadata": {
        "id": "ugyTu1xpZ4vG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86ebf52f-bebe-4319-f15e-432dcdb397e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(442, 10) (442, 1)\n",
            "[[0.66666667 1.         0.58264463 0.54929577 0.29411765 0.25697211\n",
            "  0.20779221 0.28208745 0.56221737 0.43939394]\n",
            " [0.48333333 0.         0.14876033 0.35211268 0.42156863 0.30677291\n",
            "  0.62337662 0.14104372 0.22244301 0.16666667]\n",
            " [0.88333333 1.         0.51652893 0.43661972 0.28921569 0.25896414\n",
            "  0.24675325 0.28208745 0.49658437 0.40909091]\n",
            " [0.08333333 0.         0.30165289 0.30985915 0.49509804 0.44721116\n",
            "  0.23376623 0.42313117 0.57293604 0.46969697]\n",
            " [0.51666667 0.         0.20661157 0.54929577 0.46568627 0.41733068\n",
            "  0.38961039 0.28208745 0.36236911 0.33333333]] [[0.39252336]\n",
            " [0.15576324]\n",
            " [0.36137072]\n",
            " [0.56386293]\n",
            " [0.34267913]]\n",
            "                0           1           2           3           4           5  \\\n",
            "count  442.000000  442.000000  442.000000  442.000000  442.000000  442.000000   \n",
            "mean     0.491968    0.468326    0.346107    0.459818    0.451668    0.367725   \n",
            "std      0.218484    0.499561    0.182567    0.194806    0.169647    0.151460   \n",
            "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
            "25%      0.320833    0.000000    0.214876    0.309859    0.329657    0.271165   \n",
            "50%      0.516667    0.000000    0.318182    0.436620    0.436275    0.355578   \n",
            "75%      0.666667    1.000000    0.465909    0.605634    0.552696    0.462649   \n",
            "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
            "\n",
            "                6           7           8           9  \n",
            "count  442.000000  442.000000  442.000000  442.000000  \n",
            "mean     0.360889    0.291996    0.485557    0.503942  \n",
            "std      0.167977    0.182010    0.183364    0.174187  \n",
            "min      0.000000    0.000000    0.000000    0.000000  \n",
            "25%      0.237013    0.141044    0.357528    0.382576  \n",
            "50%      0.337662    0.282087    0.478057    0.500000  \n",
            "75%      0.464286    0.423131    0.610446    0.606061  \n",
            "max      1.000000    1.000000    1.000000    1.000000                   0\n",
            "count  442.000000\n",
            "mean     0.396054\n",
            "std      0.240165\n",
            "min      0.000000\n",
            "25%      0.193146\n",
            "50%      0.359813\n",
            "75%      0.580997\n",
            "max      1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.2 全局参数设置"
      ],
      "metadata": {
        "id": "Xf0kFGsGKWna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 样本信息，划分的数据集\n",
        "N_SAMPLE = Xs_tensor.size()[0]              # 样本总数\n",
        "N_TRAIN = int(N_SAMPLE * 0.7)               # 训练样本数\n",
        "N_VALID = int(N_SAMPLE * 0.2)               # 验证样本数\n",
        "N_TEST = N_SAMPLE - N_TRAIN - N_VALID       # 测试样本数\n",
        "\n",
        "\n",
        "# 训练过程超参数设置\n",
        "BATCH_SIZE = 64\n",
        "N_EPOCH = 1000\n",
        "LEARNING_RATE = 0.05\n",
        "\n",
        "\n",
        "# 神经网络模型参数\n",
        "HIDDEN_DIM = 4\n",
        "INPUT_DIM = Xs_tensor.size()[1]     # sizes of input data and output data\n",
        "OUTPUT_DIM = Ys_tensor.size()[1]\n",
        "print(\"NN Structure:\", INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
        "\n",
        "\n",
        "# 设置 device，如果 GPU 可用，则使用\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"device:\", DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5U3xRjEKVxF",
        "outputId": "342a5a43-ba4d-4fed-d9a3-ee31adfd7e9f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN Structure: 10 4 1\n",
            "device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.3 创建结构化数据"
      ],
      "metadata": {
        "id": "FaMqo_x-Ioej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, Dataset, random_split, DataLoader\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "\n",
        "# DataSet 类\n",
        "ds = TensorDataset(Xs_tensor, Ys_tensor)\n",
        "\n",
        "# split training, validation, testing data\n",
        "ds_train, ds_vaild, ds_test = random_split(ds, lengths=[N_TRAIN, N_VALID, N_TEST])\n",
        "print(len(ds_train), len(ds_vaild), len(ds_test))\n",
        "\n",
        "\n",
        "# DataLoader of train data, valid data, test data\n",
        "dl_train = DataLoader(ds_train, batch_size = BATCH_SIZE, shuffle = True,\n",
        "    collate_fn = lambda x: tuple(x_.to(DEVICE) for x_ in default_collate(x))\n",
        "    )\n",
        "\n",
        "dl_valid = DataLoader(ds_vaild, batch_size = BATCH_SIZE, # default shuffle = False,\n",
        "    collate_fn = lambda x: tuple(x_.to(DEVICE) for x_ in default_collate(x))\n",
        "    )\n",
        "\n",
        "dl_test = DataLoader(ds_test, batch_size = BATCH_SIZE, # default shuffle = False,\n",
        "    collate_fn = lambda x: tuple(x_.to(DEVICE) for x_ in default_collate(x))\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ngWqkICIYbz",
        "outputId": "414a7ab2-9f6c-4467-b0b6-9a6687ed2d68"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "309 88 45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.4 定义模型"
      ],
      "metadata": {
        "id": "z_BqissYIaGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 方式 2: 全部使用 层类（nn.Module 类）\n",
        "class BPNNModeler2(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "\n",
        "        super(BPNNModeler2, self).__init__()\n",
        "\n",
        "        # Layer 1\n",
        "        self.layer1_linear = nn.Linear(input_dim, hidden_dim)\n",
        "        self.layer1_sigmoid = nn.Sigmoid()\n",
        "        # Layer 2\n",
        "        self.layer2_linear = nn.Linear(hidden_dim, output_dim)\n",
        "        self.layer2_sigmoid = nn.Sigmoid()\n",
        "        # Output\n",
        "        self.layer2_flattern = nn.Flatten(0, -1)\n",
        "    \n",
        "    # 正向传播\n",
        "    def forward(self, x):\n",
        "        # Layer 1\n",
        "        out_layer1 = self.layer1_linear(x)\n",
        "        out_layer1 = self.layer1_sigmoid(out_layer1)\n",
        "        # Layer 2\n",
        "        out_layer2 = self.layer2_linear(out_layer1)\n",
        "        out_layer2 = self.layer2_sigmoid(out_layer2)\n",
        "        # Output\n",
        "        out = self.layer2_flattern(out_layer2)\n",
        "        return out\n",
        "# =============================================================================="
      ],
      "metadata": {
        "id": "3AiyB78lIXa2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.4.1 初始化模型"
      ],
      "metadata": {
        "id": "qfQ_PEy3LYoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义模型\n",
        "model = BPNNModeler2(input_dim = INPUT_DIM, hidden_dim = HIDDEN_DIM, output_dim = OUTPUT_DIM)\n",
        "model = model.to(DEVICE)"
      ],
      "metadata": {
        "id": "0xniWxzQKikG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.5 定义损失函数"
      ],
      "metadata": {
        "id": "kOBsRGv-I2wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义 损失函数 MSE 损失\n",
        "def loss_func(model_out, target, reduction='mean'):\n",
        "    loss = F.mse_loss(model_out, target, reduction=reduction) \n",
        "    return loss"
      ],
      "metadata": {
        "id": "_mf3sHJ3I1C7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 基本框架"
      ],
      "metadata": {
        "id": "-p1IkD9lJBGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(torch.nn.Module):\n",
        "    def __init__(self, params):\n",
        "        # params 传入模型的参数\n",
        "        super(MyModel, self).__init__()\n",
        "\t# 放入需要学习的参数，一般由 nn.Layer() 或 nn.Parameter() 定义\n",
        "    \n",
        "    # 正向传播\n",
        "    def forward(self, x):\n",
        "        # x: 为传入的数据，第1个维度（x.size()[0]）为 batch size\n",
        "        # 根据定义的参数，确定数据的传入顺序，构建模型\n",
        "        return out\n",
        "    \n",
        "    # 损失函数\n",
        "    def loss_func(self, out, target):\n",
        "        # out：模型的输出，一般为预测值\n",
        "        # target: 输出所对应的真实值\n",
        "        return loss\n",
        "    \n",
        "    # 预测类别，用于分类模型\n",
        "    def pred_label(self, prob):\n",
        "        # 分类模型的 out 通常为 [0,1] 之间的概率形式，通过 torch.argmax() 函数也获取概率最大的标签\n",
        "        label = torch.argmax(prob, dim, keepdim=False)\n",
        "        return label"
      ],
      "metadata": {
        "id": "t5TP1MajJMwI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 方法"
      ],
      "metadata": {
        "id": "LzjMFWRPJN4m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 `torch.nn.Module()` 类主要方法"
      ],
      "metadata": {
        "id": "Q-JWvAIvJTcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 `.zero_grad()` 方法使用"
      ],
      "metadata": {
        "id": "8titi_onJYsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**实例 1**： 使用 `optimizer.zero_grad()`"
      ],
      "metadata": {
        "id": "7QYjXc8gJjBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCH = 50\n",
        "LEARNING_RATE = 0.1\n",
        "\n",
        "# 定义优化器，模型参数 model.parameters() 传入到优化器中\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)  # model: 已定义的模型\n",
        "\n",
        "for epoch in range(N_EPOCH):\n",
        "\n",
        "    train_total_loss = 0.\n",
        "\n",
        "    for i, (X_batch, Y_batch) in enumerate(dl_train):    # dl_train: 已定义的 DataLoader 类\n",
        "        optimizer.zero_grad()    # 优化器中的模型参数的梯度归零\n",
        "\n",
        "        out = model(X_batch)                           \n",
        "        loss = loss_func(out, Y_batch.flatten())  \n",
        "        loss.backward()          # 反向传播，计算梯度\n",
        "        \n",
        "        optimizer.step()         # 执行一步优化，更新参数\n",
        "\n",
        "        train_total_loss += loss.item()\n",
        "\n",
        "    # Print Traing information\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        print('Epoch: {0:>4}, Train Loss: {1:>10.5f}'.format(epoch+1, train_total_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5kdXSldJTCT",
        "outputId": "03b2b7bf-6911-4075-b619-7ff524b0bc76"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:    1, Train Loss:    0.32724\n",
            "Epoch:    2, Train Loss:    0.31922\n",
            "Epoch:    3, Train Loss:    0.31521\n",
            "Epoch:    4, Train Loss:    0.31150\n",
            "Epoch:    5, Train Loss:    0.30973\n",
            "Epoch:    6, Train Loss:    0.30414\n",
            "Epoch:    7, Train Loss:    0.30346\n",
            "Epoch:    8, Train Loss:    0.30117\n",
            "Epoch:    9, Train Loss:    0.29959\n",
            "Epoch:   10, Train Loss:    0.30005\n",
            "Epoch:   11, Train Loss:    0.29809\n",
            "Epoch:   12, Train Loss:    0.29861\n",
            "Epoch:   13, Train Loss:    0.29539\n",
            "Epoch:   14, Train Loss:    0.29791\n",
            "Epoch:   15, Train Loss:    0.29738\n",
            "Epoch:   16, Train Loss:    0.29631\n",
            "Epoch:   17, Train Loss:    0.29488\n",
            "Epoch:   18, Train Loss:    0.29493\n",
            "Epoch:   19, Train Loss:    0.29580\n",
            "Epoch:   20, Train Loss:    0.29423\n",
            "Epoch:   21, Train Loss:    0.29536\n",
            "Epoch:   22, Train Loss:    0.29538\n",
            "Epoch:   23, Train Loss:    0.29257\n",
            "Epoch:   24, Train Loss:    0.29417\n",
            "Epoch:   25, Train Loss:    0.29281\n",
            "Epoch:   26, Train Loss:    0.29741\n",
            "Epoch:   27, Train Loss:    0.29363\n",
            "Epoch:   28, Train Loss:    0.29363\n",
            "Epoch:   29, Train Loss:    0.29616\n",
            "Epoch:   30, Train Loss:    0.29388\n",
            "Epoch:   31, Train Loss:    0.29489\n",
            "Epoch:   32, Train Loss:    0.29579\n",
            "Epoch:   33, Train Loss:    0.29462\n",
            "Epoch:   34, Train Loss:    0.29507\n",
            "Epoch:   35, Train Loss:    0.29445\n",
            "Epoch:   36, Train Loss:    0.29536\n",
            "Epoch:   37, Train Loss:    0.29583\n",
            "Epoch:   38, Train Loss:    0.29466\n",
            "Epoch:   39, Train Loss:    0.29484\n",
            "Epoch:   40, Train Loss:    0.29419\n",
            "Epoch:   41, Train Loss:    0.29329\n",
            "Epoch:   42, Train Loss:    0.29490\n",
            "Epoch:   43, Train Loss:    0.29377\n",
            "Epoch:   44, Train Loss:    0.29521\n",
            "Epoch:   45, Train Loss:    0.29619\n",
            "Epoch:   46, Train Loss:    0.29276\n",
            "Epoch:   47, Train Loss:    0.29373\n",
            "Epoch:   48, Train Loss:    0.29350\n",
            "Epoch:   49, Train Loss:    0.29505\n",
            "Epoch:   50, Train Loss:    0.29329\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**实例 2**： 使用 `model.zero_grad()`"
      ],
      "metadata": {
        "id": "bfojtkvnKDgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCH = 50\n",
        "LEARNING_RATE = 0.5\n",
        "\n",
        "for epoch in range(N_EPOCH):\n",
        "\n",
        "    train_total_loss = 0.\n",
        "\n",
        "    for i, (X_batch, Y_batch) in enumerate(dl_train):   # dl_train: 已定义的 DataLoader 类\n",
        "        model.zero_grad()       # 模型参数参数梯度归零    # model: 已定义的模型\n",
        "\n",
        "        out = model(X_batch)                           \n",
        "        loss = loss_func(out, Y_batch.flatten())  \n",
        "        loss.backward()         # 反向传播，计算梯度\n",
        "\n",
        "        with torch.no_grad():   # 更新参数时，要取消梯度追踪\n",
        "            for param in model.parameters():\n",
        "                param -= LEARNING_RATE * param.grad     # 更新参数\n",
        "\n",
        "        train_total_loss += loss.item()\n",
        "\n",
        "    # Print Traing information\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        print('Epoch: {0:>4}, Train Loss: {1:>10.5f}'.format(epoch+1, train_total_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVkuQk2bJNdu",
        "outputId": "e73462fe-e01e-4a64-ea26-045a95970d79"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:    1, Train Loss:    0.29404\n",
            "Epoch:    2, Train Loss:    0.29467\n",
            "Epoch:    3, Train Loss:    0.29526\n",
            "Epoch:    4, Train Loss:    0.29565\n",
            "Epoch:    5, Train Loss:    0.29408\n",
            "Epoch:    6, Train Loss:    0.29436\n",
            "Epoch:    7, Train Loss:    0.29582\n",
            "Epoch:    8, Train Loss:    0.29346\n",
            "Epoch:    9, Train Loss:    0.29309\n",
            "Epoch:   10, Train Loss:    0.29221\n",
            "Epoch:   11, Train Loss:    0.29425\n",
            "Epoch:   12, Train Loss:    0.29461\n",
            "Epoch:   13, Train Loss:    0.29253\n",
            "Epoch:   14, Train Loss:    0.29435\n",
            "Epoch:   15, Train Loss:    0.29751\n",
            "Epoch:   16, Train Loss:    0.29161\n",
            "Epoch:   17, Train Loss:    0.29119\n",
            "Epoch:   18, Train Loss:    0.29309\n",
            "Epoch:   19, Train Loss:    0.29184\n",
            "Epoch:   20, Train Loss:    0.29282\n",
            "Epoch:   21, Train Loss:    0.29070\n",
            "Epoch:   22, Train Loss:    0.29325\n",
            "Epoch:   23, Train Loss:    0.29118\n",
            "Epoch:   24, Train Loss:    0.29331\n",
            "Epoch:   25, Train Loss:    0.29079\n",
            "Epoch:   26, Train Loss:    0.29166\n",
            "Epoch:   27, Train Loss:    0.29100\n",
            "Epoch:   28, Train Loss:    0.29277\n",
            "Epoch:   29, Train Loss:    0.29505\n",
            "Epoch:   30, Train Loss:    0.29019\n",
            "Epoch:   31, Train Loss:    0.28930\n",
            "Epoch:   32, Train Loss:    0.28823\n",
            "Epoch:   33, Train Loss:    0.29474\n",
            "Epoch:   34, Train Loss:    0.29148\n",
            "Epoch:   35, Train Loss:    0.29030\n",
            "Epoch:   36, Train Loss:    0.29137\n",
            "Epoch:   37, Train Loss:    0.29082\n",
            "Epoch:   38, Train Loss:    0.29045\n",
            "Epoch:   39, Train Loss:    0.28952\n",
            "Epoch:   40, Train Loss:    0.28757\n",
            "Epoch:   41, Train Loss:    0.28724\n",
            "Epoch:   42, Train Loss:    0.28852\n",
            "Epoch:   43, Train Loss:    0.28844\n",
            "Epoch:   44, Train Loss:    0.28961\n",
            "Epoch:   45, Train Loss:    0.28746\n",
            "Epoch:   46, Train Loss:    0.28703\n",
            "Epoch:   47, Train Loss:    0.29093\n",
            "Epoch:   48, Train Loss:    0.29006\n",
            "Epoch:   49, Train Loss:    0.28951\n",
            "Epoch:   50, Train Loss:    0.28819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 训练模式和评估模型"
      ],
      "metadata": {
        "id": "u4hg7VfhMP5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCH = 50\n",
        "LEARNING_RATE = 0.1\n",
        "\n",
        "# 定义优化器\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# 模型训练过程\n",
        "for epoch in range(N_EPOCH):\n",
        "    # 模型训练\n",
        "    train_total_loss = 0.\n",
        "\n",
        "    model.train()    # 启动模型训练模式\n",
        "    for i, (X_batch, Y_batch) in enumerate(dl_train):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        out = model(X_batch)\n",
        "        loss = loss_func(out, Y_batch.flatten())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_total_loss += loss.item()\n",
        "    \n",
        "\n",
        "    # 评估验证集 方式 1: 使用 torch.no_grad()\n",
        "    vaild_total_loss = 0.\n",
        "    \n",
        "    model.eval()    # 启动模型评估模式\n",
        "    with torch.no_grad():\n",
        "        for X_valid, Y_valid in dl_valid:\n",
        "            out = model(X_valid)\n",
        "            loss = loss_func(out, Y_valid.flatten())\n",
        "            vaild_total_loss += loss.item()\n",
        "\n",
        "\n",
        "    # 评估验证集 方式 2: 使用 .detach()\n",
        "    vaild_total_loss = 0.\n",
        "\n",
        "    model.eval()    # 启动模型评估模式\n",
        "    for X_valid, Y_valid in dl_valid:\n",
        "        # X_valid = X_valid.detach()  # 取消对 tensor 的梯度跟踪\n",
        "        # Y_valid = Y_valid.detach()\n",
        "        out = model(X_valid)\n",
        "\n",
        "        # print(X_valid.requires_grad, out.requires_grad)\n",
        "        # Output: False, True\n",
        "\n",
        "        out = out.detach() # 取消对 tensor 的梯度跟踪\n",
        "\n",
        "        loss = loss_func(out, Y_valid.flatten())\n",
        "        vaild_total_loss += loss.item()\n",
        "\n",
        "    if (epoch+1) % 1 == 0:\n",
        "        print(epoch + 1, ':', \n",
        "              round(train_total_loss, 5),\n",
        "              round(vaild_total_loss, 5))"
      ],
      "metadata": {
        "id": "7PKsqvrNcOWU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0d117ad-88ee-4db0-a8dc-00c1686f4c22"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 : 0.28681 0.12357\n",
            "2 : 0.28809 0.12349\n",
            "3 : 0.287 0.12349\n",
            "4 : 0.28772 0.12344\n",
            "5 : 0.28698 0.12342\n",
            "6 : 0.28676 0.12342\n",
            "7 : 0.28728 0.12341\n",
            "8 : 0.28575 0.12334\n",
            "9 : 0.28721 0.12331\n",
            "10 : 0.28547 0.12328\n",
            "11 : 0.28469 0.12328\n",
            "12 : 0.28674 0.12328\n",
            "13 : 0.28844 0.12327\n",
            "14 : 0.28677 0.12323\n",
            "15 : 0.28911 0.12323\n",
            "16 : 0.28702 0.12318\n",
            "17 : 0.28798 0.12315\n",
            "18 : 0.28705 0.12316\n",
            "19 : 0.28776 0.12315\n",
            "20 : 0.28626 0.12314\n",
            "21 : 0.28547 0.12311\n",
            "22 : 0.2867 0.12311\n",
            "23 : 0.28499 0.12308\n",
            "24 : 0.28765 0.12307\n",
            "25 : 0.2875 0.12305\n",
            "26 : 0.28735 0.12305\n",
            "27 : 0.28839 0.12305\n",
            "28 : 0.28704 0.12304\n",
            "29 : 0.28526 0.123\n",
            "30 : 0.28605 0.12297\n",
            "31 : 0.28476 0.12295\n",
            "32 : 0.28672 0.12293\n",
            "33 : 0.28484 0.12291\n",
            "34 : 0.28613 0.12291\n",
            "35 : 0.28785 0.12293\n",
            "36 : 0.2863 0.12292\n",
            "37 : 0.28593 0.12285\n",
            "38 : 0.28581 0.1228\n",
            "39 : 0.28784 0.12274\n",
            "40 : 0.28381 0.12274\n",
            "41 : 0.28734 0.1228\n",
            "42 : 0.28671 0.12278\n",
            "43 : 0.28671 0.12271\n",
            "44 : 0.2845 0.12267\n",
            "45 : 0.28661 0.12266\n",
            "46 : 0.28402 0.12264\n",
            "47 : 0.28694 0.12264\n",
            "48 : 0.28586 0.12261\n",
            "49 : 0.28689 0.12261\n",
            "50 : 0.28657 0.1226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 GPU 部署相关"
      ],
      "metadata": {
        "id": "ldVn_DB2NVbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 获取模型参数和模型结构"
      ],
      "metadata": {
        "id": "txrZQbXANYaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5.1 获取参数"
      ],
      "metadata": {
        "id": "Obh6DdpwNiXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### A 遍历所有参数"
      ],
      "metadata": {
        "id": "Es5TnZhpNmOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer_linear = nn.Linear(3, 5)\n",
        "\n",
        "print(\"---------.parameters() 方法-------------------\")\n",
        "for param in layer_linear.parameters():\n",
        "    print(type(param))\n",
        "    print(param.data.size())\n",
        "\n",
        "print(\"---------.named_parameters() 方法-------------\")\n",
        "for name, param in layer_linear.named_parameters():\n",
        "    print(type(param))\n",
        "    print(name, param.data.size())\n",
        "\n",
        "print(\"---------.state_dict() 方法-------------------\")\n",
        "for name, tensor in layer_linear.state_dict().items():\n",
        "    print(type(tensor))\n",
        "    print(name, tensor.size())\n",
        "\n",
        "print(\"---------.state_dict() 方法，遍历参数名--------\")\n",
        "for name in layer_linear.state_dict():\n",
        "    print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLggCJfuK6LS",
        "outputId": "131c3cb2-548b-476e-84dc-e9fba66bb1af"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------.parameters() 方法-------------------\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "torch.Size([5, 3])\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "torch.Size([5])\n",
            "---------.named_parameters() 方法-------------\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "weight torch.Size([5, 3])\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "bias torch.Size([5])\n",
            "---------.state_dict() 方法-------------------\n",
            "<class 'torch.Tensor'>\n",
            "weight torch.Size([5, 3])\n",
            "<class 'torch.Tensor'>\n",
            "bias torch.Size([5])\n",
            "---------.state_dict() 方法，遍历参数名--------\n",
            "weight\n",
            "bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "遍历无**学习参数**的 Module，输出为空"
      ],
      "metadata": {
        "id": "Zg7m8tQoOeIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer_act = nn.Sigmoid()\n",
        "\n",
        "print(\"---------.parameters() 方法-------------------\")\n",
        "for param in layer_act.parameters():\n",
        "    print(param.data.size())\n",
        "\n",
        "print(\"---------.named_parameters() 方法-------------\")\n",
        "for name, param in layer_act.named_parameters():\n",
        "    print(name, param.data.size())\n",
        "\n",
        "print(\"---------.state_dict() 方法-------------------\")\n",
        "for name, tensor in layer_act.state_dict().items():\n",
        "    print(type(tensor))\n",
        "    print(name, tensor.size())\n",
        "\n",
        "print(\"---------.state_dict() 方法，遍历参数名--------\")\n",
        "for name in layer_act.state_dict():\n",
        "    print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1U3o1s8K3ex",
        "outputId": "cb59ab1e-5895-43dd-fb17-c3b50684d535"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------.parameters() 方法-------------------\n",
            "---------.named_parameters() 方法-------------\n",
            "---------.state_dict() 方法-------------------\n",
            "---------.state_dict() 方法，遍历参数名--------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### B. 获取特定的学习参数"
      ],
      "metadata": {
        "id": "srSvTapgOWXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(layer_linear.parameters())[0].data)\n",
        "\n",
        "print(dict(layer_linear.named_parameters())['weight'].data)\n",
        "\n",
        "print(layer_linear.state_dict()['weight'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3195dGgO5iX",
        "outputId": "4477171d-0792-485b-914b-01d7cc7c0e2b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.4530, -0.3748, -0.0423],\n",
            "        [ 0.2077,  0.4028, -0.1196],\n",
            "        [ 0.2068, -0.3633, -0.1727],\n",
            "        [-0.1199, -0.1120, -0.2845],\n",
            "        [-0.5099,  0.3904,  0.4445]])\n",
            "tensor([[ 0.4530, -0.3748, -0.0423],\n",
            "        [ 0.2077,  0.4028, -0.1196],\n",
            "        [ 0.2068, -0.3633, -0.1727],\n",
            "        [-0.1199, -0.1120, -0.2845],\n",
            "        [-0.5099,  0.3904,  0.4445]])\n",
            "tensor([[ 0.4530, -0.3748, -0.0423],\n",
            "        [ 0.2077,  0.4028, -0.1196],\n",
            "        [ 0.2068, -0.3633, -0.1727],\n",
            "        [-0.1199, -0.1120, -0.2845],\n",
            "        [-0.5099,  0.3904,  0.4445]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5.2 获取模块"
      ],
      "metadata": {
        "id": "RyXZmVj7O1am"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### A 遍历所有模块"
      ],
      "metadata": {
        "id": "2VW9b_HZOzBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contain_seq = nn.Sequential(\n",
        "    nn.Linear(3, 5), \n",
        "    nn.Sigmoid(), \n",
        "    nn.Sequential(\n",
        "        nn.Linear(5, 2), \n",
        "        nn.Sigmoid()\n",
        "    ))\n",
        "\n",
        "print(\"---------.parameters() 方法-------------------\")\n",
        "for param in contain_seq.parameters():\n",
        "    print(type(param))\n",
        "    print(param.data.size())\n",
        "\n",
        "print(\"---------.named_parameters() 方法-------------\")\n",
        "for name, param in contain_seq.named_parameters():\n",
        "    print(type(param))\n",
        "    print(name, param.data.size())\n",
        "\n",
        "print(\"---------.state_dict() 方法-------------------\")\n",
        "for name, tensor in contain_seq.state_dict().items():\n",
        "    print(type(tensor))\n",
        "    print(name, tensor.size())\n",
        "\n",
        "print(\"---------.modules() 方法----------------------\")\n",
        "# 获取模型中的 所有 模块\n",
        "for module in contain_seq.modules():\n",
        "    print(type(module))\n",
        "    print(module)\n",
        "\n",
        "print(\"---------.named_modules() 方法----------------\")\n",
        "# 获取模型中的 所有 模块名 和 模块\n",
        "for name, module in contain_seq.named_modules():\n",
        "    print(type(module))\n",
        "    print(name, module)\n",
        "\n",
        "print(\"---------.named_children() 方法----------------\")\n",
        "# 获取模型中的 直接 子模块名称 和 子模块\n",
        "for name, module in contain_seq.named_children():\n",
        "    print(type(module))\n",
        "    print(name, module)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOCr2o78K5Oc",
        "outputId": "48a4a2bf-b890-4fe3-fa57-e2162de2744f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------.parameters() 方法-------------------\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "torch.Size([5, 3])\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "torch.Size([5])\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "torch.Size([2, 5])\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "torch.Size([2])\n",
            "---------.named_parameters() 方法-------------\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "0.weight torch.Size([5, 3])\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "0.bias torch.Size([5])\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "2.0.weight torch.Size([2, 5])\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "2.0.bias torch.Size([2])\n",
            "---------.state_dict() 方法-------------------\n",
            "<class 'torch.Tensor'>\n",
            "0.weight torch.Size([5, 3])\n",
            "<class 'torch.Tensor'>\n",
            "0.bias torch.Size([5])\n",
            "<class 'torch.Tensor'>\n",
            "2.0.weight torch.Size([2, 5])\n",
            "<class 'torch.Tensor'>\n",
            "2.0.bias torch.Size([2])\n",
            "---------.modules() 方法----------------------\n",
            "<class 'torch.nn.modules.container.Sequential'>\n",
            "Sequential(\n",
            "  (0): Linear(in_features=3, out_features=5, bias=True)\n",
            "  (1): Sigmoid()\n",
            "  (2): Sequential(\n",
            "    (0): Linear(in_features=5, out_features=2, bias=True)\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            ")\n",
            "<class 'torch.nn.modules.linear.Linear'>\n",
            "Linear(in_features=3, out_features=5, bias=True)\n",
            "<class 'torch.nn.modules.activation.Sigmoid'>\n",
            "Sigmoid()\n",
            "<class 'torch.nn.modules.container.Sequential'>\n",
            "Sequential(\n",
            "  (0): Linear(in_features=5, out_features=2, bias=True)\n",
            "  (1): Sigmoid()\n",
            ")\n",
            "<class 'torch.nn.modules.linear.Linear'>\n",
            "Linear(in_features=5, out_features=2, bias=True)\n",
            "<class 'torch.nn.modules.activation.Sigmoid'>\n",
            "Sigmoid()\n",
            "---------.named_modules() 方法----------------\n",
            "<class 'torch.nn.modules.container.Sequential'>\n",
            " Sequential(\n",
            "  (0): Linear(in_features=3, out_features=5, bias=True)\n",
            "  (1): Sigmoid()\n",
            "  (2): Sequential(\n",
            "    (0): Linear(in_features=5, out_features=2, bias=True)\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            ")\n",
            "<class 'torch.nn.modules.linear.Linear'>\n",
            "0 Linear(in_features=3, out_features=5, bias=True)\n",
            "<class 'torch.nn.modules.activation.Sigmoid'>\n",
            "1 Sigmoid()\n",
            "<class 'torch.nn.modules.container.Sequential'>\n",
            "2 Sequential(\n",
            "  (0): Linear(in_features=5, out_features=2, bias=True)\n",
            "  (1): Sigmoid()\n",
            ")\n",
            "<class 'torch.nn.modules.linear.Linear'>\n",
            "2.0 Linear(in_features=5, out_features=2, bias=True)\n",
            "<class 'torch.nn.modules.activation.Sigmoid'>\n",
            "2.1 Sigmoid()\n",
            "---------.named_children() 方法----------------\n",
            "<class 'torch.nn.modules.linear.Linear'>\n",
            "0 Linear(in_features=3, out_features=5, bias=True)\n",
            "<class 'torch.nn.modules.activation.Sigmoid'>\n",
            "1 Sigmoid()\n",
            "<class 'torch.nn.modules.container.Sequential'>\n",
            "2 Sequential(\n",
            "  (0): Linear(in_features=5, out_features=2, bias=True)\n",
            "  (1): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### B. 获取指定的模块"
      ],
      "metadata": {
        "id": "63WtB77HPBIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(contain_seq.modules())[0])\n",
        "\n",
        "print(dict(contain_seq.named_modules())['2.0'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFMj9XumOrz9",
        "outputId": "eccb1902-6b75-4b46-ff76-c41d3b669f8b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=3, out_features=5, bias=True)\n",
            "  (1): Sigmoid()\n",
            "  (2): Sequential(\n",
            "    (0): Linear(in_features=5, out_features=2, bias=True)\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            ")\n",
            "Linear(in_features=5, out_features=2, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### C. 获取指定模块的学习参数"
      ],
      "metadata": {
        "id": "2bzlLfCQPJmA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6 关闭梯度（冻结层）"
      ],
      "metadata": {
        "id": "SoFOisOUPQbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 实例：BPNN 神经网络的 4 种构建方法及分析"
      ],
      "metadata": {
        "id": "ueIiRnGDPbU4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 BPNN 神经网络的 4 种构建方法"
      ],
      "metadata": {
        "id": "Mnxcy200PeI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "定义一个 单隐层神经网络，4 种方式\n",
        "\n",
        "- 方式 1: 使用 `nn.Sequential()`\n",
        "\n",
        "- 方式 2: 全部使用 层类（`nn.Module` 类）\n",
        "\n",
        "- 方式 3：使用函数类型：`torch.sigmoid()` 和 `torch.flatten()`\n",
        "\n",
        "- 方式 4：使用 `nn.Parameter()`"
      ],
      "metadata": {
        "id": "FwE4PmOYFImb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### A 方式 1：使用 `nn.Sequential()`"
      ],
      "metadata": {
        "id": "3zmsxqYSPlkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 方式 1: 使用 nn.Sequential()\n",
        "class BPNNModeler(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "\n",
        "        super(BPNNModeler, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim), \n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(hidden_dim, output_dim), \n",
        "            nn.Sigmoid(),\n",
        "            nn.Flatten(0, -1)\n",
        "        )\n",
        "    \n",
        "    # 正向传播\n",
        "    def forward(self, x):\n",
        "        out = self.model(x)\n",
        "        return out\n",
        "# =============================================================================="
      ],
      "metadata": {
        "id": "NjZibFRIPnMk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### B 方式 2：全部使用 层类"
      ],
      "metadata": {
        "id": "xPR132YePnob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 方式 2: 全部使用 层类（nn.Module 类）\n",
        "class BPNNModeler2(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "\n",
        "        super(BPNNModeler2, self).__init__()\n",
        "\n",
        "        # Layer 1\n",
        "        self.layer1_linear = nn.Linear(input_dim, hidden_dim)\n",
        "        self.layer1_sigmoid = nn.Sigmoid()\n",
        "        # Layer 2\n",
        "        self.layer2_linear = nn.Linear(hidden_dim, output_dim)\n",
        "        self.layer2_sigmoid = nn.Sigmoid()\n",
        "        # Output\n",
        "        self.layer2_flattern = nn.Flatten(0, -1)\n",
        "    \n",
        "    # 正向传播\n",
        "    def forward(self, x):\n",
        "        # Layer 1\n",
        "        out_layer1 = self.layer1_linear(x)\n",
        "        out_layer1 = self.layer1_sigmoid(out_layer1)\n",
        "        # Layer 2\n",
        "        out_layer2 = self.layer2_linear(out_layer1)\n",
        "        out_layer2 = self.layer2_sigmoid(out_layer2)\n",
        "        # Output\n",
        "        out = self.layer2_flattern(out_layer2)\n",
        "        return out\n",
        "# =============================================================================="
      ],
      "metadata": {
        "id": "5hfUsHB7PxuX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### C 方式 3：使用函数类型：`torch.sigmoid()` 和 `torch.flatten()`"
      ],
      "metadata": {
        "id": "DXX_G2JsPx0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 方式 3：使用函数类型：torch.sigmoid() 和 torch.flatten()\n",
        "class BPNNModeler3(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "\n",
        "        super(BPNNModeler3, self).__init__()\n",
        "        # Layer 1\n",
        "        self.layer1_linear = nn.Linear(input_dim, hidden_dim)\n",
        "        # Layer 2\n",
        "        self.layer2_linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    # 正向传播\n",
        "    def forward(self, x):\n",
        "        # Layer 1\n",
        "        out_layer1 = torch.sigmoid(self.layer1_linear(x))\n",
        "        # Layer 2\n",
        "        out_layer2 = torch.sigmoid(self.layer2_linear(out_layer1))\n",
        "        # Output\n",
        "        out = torch.flatten(out_layer2)\n",
        "        return out\n",
        "# =============================================================================="
      ],
      "metadata": {
        "id": "wIZDUA54P1YW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### D 方式 4：使用 `nn.Parameter()`"
      ],
      "metadata": {
        "id": "y-VRvOx0P1fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 方式 4：使用 nn.Parameter()\n",
        "class BPNNModeler4(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "\n",
        "        super(BPNNModeler4, self).__init__()\n",
        "        # Layer 1\n",
        "        self.w1 = nn.Parameter(torch.rand((input_dim, hidden_dim)))\n",
        "        self.b1 = nn.Parameter(torch.rand(hidden_dim))\n",
        "        # Layer 2\n",
        "        self.w2 = nn.Parameter(torch.rand((hidden_dim, output_dim)))\n",
        "        self.b2 = nn.Parameter(torch.rand(hidden_dim))\n",
        "    \n",
        "    # 正向传播\n",
        "    def forward(self, x):\n",
        "        # Layer 1\n",
        "        out_layer1 = torch.mm(x, self.w1) + self.b1\n",
        "        out_layer1 = torch.sigmoid(out_layer1)\n",
        "        # Layer 2\n",
        "        out_layer2 = torch.mm(out_layer1, self.w2) + self.b2\n",
        "        out_layer2 = torch.sigmoid(out_layer2)\n",
        "        # Output\n",
        "        out = torch.flatten(out_layer2)\n",
        "        return out\n",
        "# =============================================================================="
      ],
      "metadata": {
        "id": "hjTURqMwPI9X"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 区别"
      ],
      "metadata": {
        "id": "4Se5GGtnQONl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_info(model):\n",
        "\n",
        "    print(\"---------.parameters() 方法-------------------\")\n",
        "    for param in model.parameters():\n",
        "        print(type(param), param.data.size())\n",
        "\n",
        "    print(\"---------.named_parameters() 方法-------------\")\n",
        "    for name, param in model.named_parameters():\n",
        "        print(type(param), name, param.data.size())\n",
        "    \n",
        "    print(\"---------.state_dict() 方法-------------------\")\n",
        "    for name, tensor in model.state_dict().items():\n",
        "        print(type(tensor), name, tensor.size())\n",
        "\n",
        "    print(\"---------.modules() 方法----------------------\")\n",
        "    for module in model.modules():\n",
        "        print(type(module), module)\n",
        "\n",
        "    print(\"---------.named_modules() 方法----------------\")\n",
        "    for name, module in model.named_modules():\n",
        "        print(type(module), name, module)\n",
        "\n",
        "    print(\"---------.named_children() 方法----------------\")\n",
        "    for name, module in model.named_children():\n",
        "        print(type(module), name, module)"
      ],
      "metadata": {
        "id": "N3I92909Yi_7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 神经网络模型参数\n",
        "HIDDEN_DIM = 10\n",
        "INPUT_DIM = 5     \n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "\n",
        "# 实例化一个神经网络模型\n",
        "model = BPNNModeler(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, output_dim=OUTPUT_DIM)\n",
        "\n",
        "model2 = BPNNModeler2(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, output_dim=OUTPUT_DIM)\n",
        "\n",
        "model3 = BPNNModeler3(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, output_dim=OUTPUT_DIM)\n",
        "\n",
        "model4 = BPNNModeler4(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, output_dim=OUTPUT_DIM)\n",
        "\n",
        "\n",
        "print_info(model)\n",
        "\n",
        "print_info(model2)\n",
        "\n",
        "print_info(model3)\n",
        "\n",
        "print_info(model4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pb-eZXZFFPUe",
        "outputId": "980d0265-ffc9-46a0-acfc-61358df45ee1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------.parameters() 方法-------------------\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([10, 5])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([10])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([1, 10])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([1])\n",
            "---------.named_parameters() 方法-------------\n",
            "<class 'torch.nn.parameter.Parameter'> model.0.weight torch.Size([10, 5])\n",
            "<class 'torch.nn.parameter.Parameter'> model.0.bias torch.Size([10])\n",
            "<class 'torch.nn.parameter.Parameter'> model.2.weight torch.Size([1, 10])\n",
            "<class 'torch.nn.parameter.Parameter'> model.2.bias torch.Size([1])\n",
            "---------.state_dict() 方法-------------------\n",
            "<class 'torch.Tensor'> model.0.weight torch.Size([10, 5])\n",
            "<class 'torch.Tensor'> model.0.bias torch.Size([10])\n",
            "<class 'torch.Tensor'> model.2.weight torch.Size([1, 10])\n",
            "<class 'torch.Tensor'> model.2.bias torch.Size([1])\n",
            "---------.modules() 方法----------------------\n",
            "<class '__main__.BPNNModeler'> BPNNModeler(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=5, out_features=10, bias=True)\n",
            "    (1): Sigmoid()\n",
            "    (2): Linear(in_features=10, out_features=1, bias=True)\n",
            "    (3): Sigmoid()\n",
            "    (4): Flatten(start_dim=0, end_dim=-1)\n",
            "  )\n",
            ")\n",
            "<class 'torch.nn.modules.container.Sequential'> Sequential(\n",
            "  (0): Linear(in_features=5, out_features=10, bias=True)\n",
            "  (1): Sigmoid()\n",
            "  (2): Linear(in_features=10, out_features=1, bias=True)\n",
            "  (3): Sigmoid()\n",
            "  (4): Flatten(start_dim=0, end_dim=-1)\n",
            ")\n",
            "<class 'torch.nn.modules.linear.Linear'> Linear(in_features=5, out_features=10, bias=True)\n",
            "<class 'torch.nn.modules.activation.Sigmoid'> Sigmoid()\n",
            "<class 'torch.nn.modules.linear.Linear'> Linear(in_features=10, out_features=1, bias=True)\n",
            "<class 'torch.nn.modules.activation.Sigmoid'> Sigmoid()\n",
            "<class 'torch.nn.modules.flatten.Flatten'> Flatten(start_dim=0, end_dim=-1)\n",
            "---------.named_modules() 方法----------------\n",
            "<class '__main__.BPNNModeler'>  BPNNModeler(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=5, out_features=10, bias=True)\n",
            "    (1): Sigmoid()\n",
            "    (2): Linear(in_features=10, out_features=1, bias=True)\n",
            "    (3): Sigmoid()\n",
            "    (4): Flatten(start_dim=0, end_dim=-1)\n",
            "  )\n",
            ")\n",
            "<class 'torch.nn.modules.container.Sequential'> model Sequential(\n",
            "  (0): Linear(in_features=5, out_features=10, bias=True)\n",
            "  (1): Sigmoid()\n",
            "  (2): Linear(in_features=10, out_features=1, bias=True)\n",
            "  (3): Sigmoid()\n",
            "  (4): Flatten(start_dim=0, end_dim=-1)\n",
            ")\n",
            "<class 'torch.nn.modules.linear.Linear'> model.0 Linear(in_features=5, out_features=10, bias=True)\n",
            "<class 'torch.nn.modules.activation.Sigmoid'> model.1 Sigmoid()\n",
            "<class 'torch.nn.modules.linear.Linear'> model.2 Linear(in_features=10, out_features=1, bias=True)\n",
            "<class 'torch.nn.modules.activation.Sigmoid'> model.3 Sigmoid()\n",
            "<class 'torch.nn.modules.flatten.Flatten'> model.4 Flatten(start_dim=0, end_dim=-1)\n",
            "---------.named_children() 方法----------------\n",
            "<class 'torch.nn.modules.container.Sequential'> model Sequential(\n",
            "  (0): Linear(in_features=5, out_features=10, bias=True)\n",
            "  (1): Sigmoid()\n",
            "  (2): Linear(in_features=10, out_features=1, bias=True)\n",
            "  (3): Sigmoid()\n",
            "  (4): Flatten(start_dim=0, end_dim=-1)\n",
            ")\n",
            "---------.parameters() 方法-------------------\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([10, 5])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([10])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([1, 10])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([1])\n",
            "---------.named_parameters() 方法-------------\n",
            "<class 'torch.nn.parameter.Parameter'> layer1_linear.weight torch.Size([10, 5])\n",
            "<class 'torch.nn.parameter.Parameter'> layer1_linear.bias torch.Size([10])\n",
            "<class 'torch.nn.parameter.Parameter'> layer2_linear.weight torch.Size([1, 10])\n",
            "<class 'torch.nn.parameter.Parameter'> layer2_linear.bias torch.Size([1])\n",
            "---------.state_dict() 方法-------------------\n",
            "<class 'torch.Tensor'> layer1_linear.weight torch.Size([10, 5])\n",
            "<class 'torch.Tensor'> layer1_linear.bias torch.Size([10])\n",
            "<class 'torch.Tensor'> layer2_linear.weight torch.Size([1, 10])\n",
            "<class 'torch.Tensor'> layer2_linear.bias torch.Size([1])\n",
            "---------.modules() 方法----------------------\n",
            "<class '__main__.BPNNModeler2'> BPNNModeler2(\n",
            "  (layer1_linear): Linear(in_features=5, out_features=10, bias=True)\n",
            "  (layer1_sigmoid): Sigmoid()\n",
            "  (layer2_linear): Linear(in_features=10, out_features=1, bias=True)\n",
            "  (layer2_sigmoid): Sigmoid()\n",
            "  (layer2_flattern): Flatten(start_dim=0, end_dim=-1)\n",
            ")\n",
            "<class 'torch.nn.modules.linear.Linear'> Linear(in_features=5, out_features=10, bias=True)\n",
            "<class 'torch.nn.modules.activation.Sigmoid'> Sigmoid()\n",
            "<class 'torch.nn.modules.linear.Linear'> Linear(in_features=10, out_features=1, bias=True)\n",
            "<class 'torch.nn.modules.activation.Sigmoid'> Sigmoid()\n",
            "<class 'torch.nn.modules.flatten.Flatten'> Flatten(start_dim=0, end_dim=-1)\n",
            "---------.named_modules() 方法----------------\n",
            "<class '__main__.BPNNModeler2'>  BPNNModeler2(\n",
            "  (layer1_linear): Linear(in_features=5, out_features=10, bias=True)\n",
            "  (layer1_sigmoid): Sigmoid()\n",
            "  (layer2_linear): Linear(in_features=10, out_features=1, bias=True)\n",
            "  (layer2_sigmoid): Sigmoid()\n",
            "  (layer2_flattern): Flatten(start_dim=0, end_dim=-1)\n",
            ")\n",
            "<class 'torch.nn.modules.linear.Linear'> layer1_linear Linear(in_features=5, out_features=10, bias=True)\n",
            "<class 'torch.nn.modules.activation.Sigmoid'> layer1_sigmoid Sigmoid()\n",
            "<class 'torch.nn.modules.linear.Linear'> layer2_linear Linear(in_features=10, out_features=1, bias=True)\n",
            "<class 'torch.nn.modules.activation.Sigmoid'> layer2_sigmoid Sigmoid()\n",
            "<class 'torch.nn.modules.flatten.Flatten'> layer2_flattern Flatten(start_dim=0, end_dim=-1)\n",
            "---------.named_children() 方法----------------\n",
            "<class 'torch.nn.modules.linear.Linear'> layer1_linear Linear(in_features=5, out_features=10, bias=True)\n",
            "<class 'torch.nn.modules.activation.Sigmoid'> layer1_sigmoid Sigmoid()\n",
            "<class 'torch.nn.modules.linear.Linear'> layer2_linear Linear(in_features=10, out_features=1, bias=True)\n",
            "<class 'torch.nn.modules.activation.Sigmoid'> layer2_sigmoid Sigmoid()\n",
            "<class 'torch.nn.modules.flatten.Flatten'> layer2_flattern Flatten(start_dim=0, end_dim=-1)\n",
            "---------.parameters() 方法-------------------\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([10, 5])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([10])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([1, 10])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([1])\n",
            "---------.named_parameters() 方法-------------\n",
            "<class 'torch.nn.parameter.Parameter'> layer1_linear.weight torch.Size([10, 5])\n",
            "<class 'torch.nn.parameter.Parameter'> layer1_linear.bias torch.Size([10])\n",
            "<class 'torch.nn.parameter.Parameter'> layer2_linear.weight torch.Size([1, 10])\n",
            "<class 'torch.nn.parameter.Parameter'> layer2_linear.bias torch.Size([1])\n",
            "---------.state_dict() 方法-------------------\n",
            "<class 'torch.Tensor'> layer1_linear.weight torch.Size([10, 5])\n",
            "<class 'torch.Tensor'> layer1_linear.bias torch.Size([10])\n",
            "<class 'torch.Tensor'> layer2_linear.weight torch.Size([1, 10])\n",
            "<class 'torch.Tensor'> layer2_linear.bias torch.Size([1])\n",
            "---------.modules() 方法----------------------\n",
            "<class '__main__.BPNNModeler3'> BPNNModeler3(\n",
            "  (layer1_linear): Linear(in_features=5, out_features=10, bias=True)\n",
            "  (layer2_linear): Linear(in_features=10, out_features=1, bias=True)\n",
            ")\n",
            "<class 'torch.nn.modules.linear.Linear'> Linear(in_features=5, out_features=10, bias=True)\n",
            "<class 'torch.nn.modules.linear.Linear'> Linear(in_features=10, out_features=1, bias=True)\n",
            "---------.named_modules() 方法----------------\n",
            "<class '__main__.BPNNModeler3'>  BPNNModeler3(\n",
            "  (layer1_linear): Linear(in_features=5, out_features=10, bias=True)\n",
            "  (layer2_linear): Linear(in_features=10, out_features=1, bias=True)\n",
            ")\n",
            "<class 'torch.nn.modules.linear.Linear'> layer1_linear Linear(in_features=5, out_features=10, bias=True)\n",
            "<class 'torch.nn.modules.linear.Linear'> layer2_linear Linear(in_features=10, out_features=1, bias=True)\n",
            "---------.named_children() 方法----------------\n",
            "<class 'torch.nn.modules.linear.Linear'> layer1_linear Linear(in_features=5, out_features=10, bias=True)\n",
            "<class 'torch.nn.modules.linear.Linear'> layer2_linear Linear(in_features=10, out_features=1, bias=True)\n",
            "---------.parameters() 方法-------------------\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([5, 10])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([10])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([10, 1])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([10])\n",
            "---------.named_parameters() 方法-------------\n",
            "<class 'torch.nn.parameter.Parameter'> w1 torch.Size([5, 10])\n",
            "<class 'torch.nn.parameter.Parameter'> b1 torch.Size([10])\n",
            "<class 'torch.nn.parameter.Parameter'> w2 torch.Size([10, 1])\n",
            "<class 'torch.nn.parameter.Parameter'> b2 torch.Size([10])\n",
            "---------.state_dict() 方法-------------------\n",
            "<class 'torch.Tensor'> w1 torch.Size([5, 10])\n",
            "<class 'torch.Tensor'> b1 torch.Size([10])\n",
            "<class 'torch.Tensor'> w2 torch.Size([10, 1])\n",
            "<class 'torch.Tensor'> b2 torch.Size([10])\n",
            "---------.modules() 方法----------------------\n",
            "<class '__main__.BPNNModeler4'> BPNNModeler4()\n",
            "---------.named_modules() 方法----------------\n",
            "<class '__main__.BPNNModeler4'>  BPNNModeler4()\n",
            "---------.named_children() 方法----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 评估验证集的损失函数"
      ],
      "metadata": {
        "id": "kyB5K6SNfyfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 神经网络模型参数\n",
        "HIDDEN_DIM = 4\n",
        "INPUT_DIM = Xs_tensor.size()[1]     # sizes of input data and output data\n",
        "OUTPUT_DIM = Ys_tensor.size()[1]\n",
        "\n",
        "# 定义模型\n",
        "model = BPNNModeler2(input_dim = INPUT_DIM, hidden_dim = HIDDEN_DIM, output_dim = OUTPUT_DIM)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "\n",
        "# 方式 1：\n",
        "valid_total_loss = 0. \n",
        "with torch.no_grad():  #  取消梯度跟踪\n",
        "    for X_valid, Y_valid in dl_valid:              # dl_valid: 需要定义的 dataloader 类\n",
        "        out = model(X_valid)                       # model: 需要定义的 nn.Model 类\n",
        "        print(out.requires_grad)\n",
        "\n",
        "        loss = loss_func(out, Y_valid.flatten())   # loss_func：损失函数\n",
        "        valid_total_loss += loss.item()            # .item() 方法：将tensor类型转换为 python 原生的数字数据类型，只用于只有一个元素的tensor\n",
        "\n",
        "# 方式 2：\n",
        "valid_total_loss = 0. \n",
        "for X_valid, Y_valid in dl_valid:   # 从 DataLoader 中取出的 tensor，梯度是关闭的（即 .requires_grad = False）   \n",
        "    \n",
        "    out_t = model(X_valid)\n",
        "    out = out_t.detach()   # 由于模型学习参数（即 model.parameters()）的梯度是打开的，所以，计算结果（out）的梯度也是打开的，需要通过 .detach() 返回关闭梯度的 tensor8\n",
        "    print(out_t.requires_grad, out.requires_grad)\n",
        "\n",
        "    loss = loss_func(out, Y_valid.flatten())\n",
        "    valid_total_loss += loss.item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDtPT9Bzf337",
        "outputId": "75c9419d-6143-474b-8e8b-f7ff5e71a594"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "False\n",
            "True False\n",
            "True False\n"
          ]
        }
      ]
    }
  ]
}