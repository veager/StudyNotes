{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP 入门-分词-词频统计.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPJTz1TXKlLxLBmmIJ9tz6D"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP 入门\n",
        "\n",
        "使用的库\n",
        "\n",
        "- `NTKL` 库\n",
        "- `jieba` 结巴分词\n",
        "- `scikit-learn` 库下，`sklearn.feature_extraction.text` 模块\n",
        "\n",
        "主要内容\n",
        "\n",
        "- 分词\n",
        "\n",
        "- tf, idf, tf-idf\n",
        "\n",
        "参考资料\n",
        "\n",
        "- NLP 基础：分词，停词，n元语法, 博客园, [website](https://www.cnblogs.com/veager/articles/16288751.html)\n",
        "\n",
        "- 文本特征提取, 博客园, [website](https://www.cnblogs.com/veager/articles/16285476.html)"
      ],
      "metadata": {
        "id": "bcVboio-gn3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "M2VWwPxTV7X6"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 分词"
      ],
      "metadata": {
        "id": "wVlt9cbchGDI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 `NLTK` 库"
      ],
      "metadata": {
        "id": "nQKDfM85hMmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "08DP-Sfgh3nu"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 分词所必要的数据\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGxOdTgJh6Rl",
        "outputId": "9bbfb80b-4085-47ec-93b8-35a9c6f8ac3e"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
        "\n",
        "s = 'Good muffins cost $3.88\\nin New York. Please buy me ... two of them.\\n\\nThanks.'\n",
        "\n",
        "tokens_1 = word_tokenize(s)    # tokens\n",
        "tokens_2 = wordpunct_tokenize(s)\n",
        "print(tokens_1)\n",
        "print(tokens_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-GX08nzhLJI",
        "outputId": "45483047-2da2-4260-b694-41b5bb1e301a"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', '...', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
            "['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', '...', 'two', 'of', 'them', '.', 'Thanks', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 `jieba` 分词"
      ],
      "metadata": {
        "id": "RjhGjmlPj3l7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jieba\n",
        "\n",
        "doc = \"我来到北京清华大学\"\n",
        "seg_list = jieba.cut(doc, cut_all=True)\n",
        "print(\"Full Mode: \" + \"/ \".join(seg_list))  # 全模式\n",
        "\n",
        "seg_list = jieba.cut(doc, cut_all=False)\n",
        "print(\"Default Mode: \" + \"/ \".join(seg_list))  # 精确模式"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_hhBwSpGB-8",
        "outputId": "fceeed08-e47b-4a90-d784-566247e6d2c6"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Mode: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学\n",
            "Default Mode: 我/ 来到/ 北京/ 清华大学\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jieba.del_word(\"创新办\")\n",
        "jieba.del_word(\"云计算\")\n",
        "doc= \"李小福是创新办主任也是云计算方面的专家\"\n",
        "seg_list = jieba.cut(doc)\n",
        "print( \"/ \".join(seg_list))\n",
        "\n",
        "jieba.add_word(\"创新办\")\n",
        "jieba.add_word(\"云计算\")\n",
        "seg_list = jieba.cut(doc)\n",
        "print(\"/ \".join(seg_list))"
      ],
      "metadata": {
        "id": "0pe0xXRFjvWg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "550831ce-b766-4b2a-a84e-455d58159521"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "李小福/ 是/ 创新/ 办/ 主任/ 也/ 是/ 云/ 计算/ 方面/ 的/ 专家\n",
            "李小福/ 是/ 创新办/ 主任/ 也/ 是/ 云计算/ 方面/ 的/ 专家\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 停词"
      ],
      "metadata": {
        "id": "irQkNuXMhfBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 `NLTK` 库"
      ],
      "metadata": {
        "id": "vEQAa1V3i4BS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7A0FXl04iv8Z",
        "outputId": "1c7a2ef3-e29f-4328-c31f-f182c90f271f"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)\n",
        "print(len(stop_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmdeSiZkgvJF",
        "outputId": "1bb55a93-1e41-42ee-dd7a-806aad1df231"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'further', 'won', 'only', \"should've\", 'we', \"it's\", 'while', 'under', 'whom', 'other', 'ours', 'she', \"don't\", 'in', 'so', \"needn't\", 'any', 'there', \"hasn't\", 'my', 'during', 'because', 'has', 'o', 'do', \"wouldn't\", 'had', 'few', 'its', 'such', 'mightn', 'at', 'out', 'should', 'than', 'not', 'here', 'on', 'isn', 'just', 'was', 'then', 'for', 'you', 'until', 'no', 'herself', 'or', 'once', 'needn', 'themselves', 'but', \"you've\", 'him', 'having', 'where', 'aren', 'own', \"doesn't\", 'over', \"couldn't\", 'when', 'same', 'll', 'with', 'after', 'your', 'm', \"shan't\", 'ain', 'itself', 'between', 'this', 'been', 'from', 'they', 'to', 'and', 'am', \"shouldn't\", 'some', 'me', 'as', 'hasn', 'our', 'if', 'of', 'were', 'doing', 't', 'mustn', 'above', 'below', 'both', 'weren', 'wouldn', 'he', 'by', 'ma', 'don', \"haven't\", 'ourselves', \"isn't\", \"mightn't\", 'd', 'yourself', 'her', \"won't\", 'are', 's', \"that'll\", 'against', 'an', 'wasn', 'who', 'off', \"weren't\", 'being', 'hers', 'have', 'those', 'up', 'theirs', 'what', 'how', 'that', 'again', 'doesn', 'yours', 'couldn', 'into', 'will', 'before', 'is', 'about', \"you'd\", 'them', 'more', \"you're\", 'yourselves', 'which', 'can', \"you'll\", 'their', 'does', 'all', 'down', \"wasn't\", 'it', 'each', 'very', \"mustn't\", 'be', 'these', 'did', 'shan', 'himself', 've', 'hadn', 'haven', \"hadn't\", \"aren't\", \"didn't\", 'his', 'now', 'too', 'i', \"she's\", 'myself', 'a', 'through', 'why', 'nor', 'didn', 're', 'y', 'shouldn', 'most', 'the'}\n",
            "179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 过滤停词"
      ],
      "metadata": {
        "id": "0nGbUySEkNK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens_1)\n",
        "\n",
        "filtered_tokens = [w for w in tokens_1 if not w in stopwords.words()]\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TX843885kSYi",
        "outputId": "60b17021-010b-47c9-afa2-2ec446d2217c"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', '...', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
            "['Good', 'muffins', 'cost', '$', '3.88', 'New', 'York', '.', 'Please', 'buy', '...', 'two', '.', 'Thanks', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 `scikit-learn` 库"
      ],
      "metadata": {
        "id": "crG3Il6TjiEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "corpus = ['This is the first document']\n",
        "stop_words = vectorizer.fit(corpus).get_stop_words()\n",
        "\n",
        "print(stop_words)\n",
        "print(len(stop_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P82ivLmkhexz",
        "outputId": "7f32c189-6ccb-4fc1-bc5f-e6ecf5e71099"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "frozenset({'we', 'other', 'under', 'ours', 'within', 'latterly', 'my', 'hundred', 'everyone', 'here', 'on', 'ten', 'then', 'everywhere', 'or', 'seemed', 'anywhere', 'still', 'own', 'over', 'upon', 'sometimes', 'after', 'itself', 'onto', 'twelve', 'eight', 'bill', 'less', 'me', 'fifty', 'else', 'due', 'cannot', 'seeming', 'became', 'elsewhere', 'inc', 'find', 'beside', 'whence', 'are', 'cry', 'back', 'part', 'wherever', 'up', 'those', 'what', 'how', 'since', 'whither', 'before', 'mill', 'can', 'becoming', 'might', 'put', 'whereby', 'hereby', 'etc', 'each', 'formerly', 'these', 'much', 'least', 'through', 'go', 'cant', 'the', 'see', 'although', 'only', 'she', 'in', 'several', 'sixty', 'move', 'during', 'do', 'few', 'had', 'such', 'empty', 'at', 'interest', 'bottom', 'than', 'not', 'was', 'many', 'another', 'for', 'behind', 'whose', 'de', 'becomes', 'but', 'him', 'six', 'where', 'nowhere', 'five', 'somewhere', 'serious', 'with', 'every', 'show', 'beforehand', 'between', 'together', 'from', 'thereupon', 'amount', 'anything', 'to', 'forty', 'as', 'our', 'though', 'become', 'of', 'take', 'co', 'except', 'below', 'by', 'name', 'un', 'fifteen', 'full', 'would', 'against', 'off', 'give', 'along', 'that', 'ie', 'whoever', 'yours', 'neither', 'will', 'found', 'them', 'ltd', 'their', 'eleven', 'down', 'none', 'whereafter', 'alone', 'detail', 'couldnt', 'otherwise', 'thin', 'hasnt', 'his', 'yet', 'fire', 'i', 'therein', 'myself', 'must', 'around', 'whom', 'meanwhile', 'any', 'us', 'there', 'therefore', 'without', 'front', 'thereby', 'has', 'twenty', 'among', 'everything', 'out', 'should', 'hereupon', 'afterwards', 'latter', 'you', 'also', 'no', 'until', 'herself', 'either', 'amoungst', 'perhaps', 'whereas', 'could', 'anyway', 'anyhow', 'towards', 'often', 'this', 'four', 'ever', 'noone', 'whatever', 'thereafter', 'top', 'via', 'her', 'may', 'mine', 'always', 'somehow', 'who', 'per', 'have', 'something', 'side', 'about', 'however', 'more', 'which', 'yourselves', 'system', 'two', 'sincere', 'whole', 'be', 'fill', 'someone', 'across', 'throughout', 'eg', 'whether', 'besides', 'now', 'too', 'nor', 'whenever', 'get', 'most', 'further', 'already', 'while', 'nothing', 'so', 'nobody', 'because', 'enough', 'its', 'mostly', 'nevertheless', 'never', 'once', 'first', 'themselves', 'call', 'seem', 'when', 'same', 'moreover', 'even', 'your', 'wherein', 'describe', 'beyond', 'been', 'well', 'they', 'and', 'anyone', 'rather', 'thick', 'seems', 'am', 'some', 'done', 'if', 'above', 'both', 'he', 'others', 'namely', 'next', 'ourselves', 'hence', 'one', 'hereafter', 'yourself', 'almost', 'amongst', 'keep', 'last', 'an', 'please', 'being', 'hers', 'thence', 'again', 'three', 'nine', 'into', 'is', 'former', 'con', 'all', 'it', 'very', 'third', 'indeed', 'thus', 'himself', 'made', 'toward', 'herein', 'sometime', 'thru', 'a', 'why', 're', 'whereupon', 'were'})\n",
            "318\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. n-gram"
      ],
      "metadata": {
        "id": "NPOqzP2Kk5Pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams, trigrams, bigrams\n",
        "\n",
        "tokens = \"Insurgents killed in ongoing fighting\".split()\n",
        "\n",
        "trigrams_1 = ngrams(tokens, n=3, pad_left=True, left_pad_symbol='</s>')\n",
        "trigrams_2 = trigrams(tokens, pad_left=True, left_pad_symbol='</s>')\n",
        "\n",
        "print(list(tokens))\n",
        "print(list(trigrams_1))     # 得到的 trigrams_1 和 trigrams_2 相同\n",
        "print(list(trigrams_2))"
      ],
      "metadata": {
        "id": "2GV44PMMk-g-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac541e28-5823-4260-de1a-dc0533a9ece0"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Insurgents', 'killed', 'in', 'ongoing', 'fighting']\n",
            "[('</s>', '</s>', 'Insurgents'), ('</s>', 'Insurgents', 'killed'), ('Insurgents', 'killed', 'in'), ('killed', 'in', 'ongoing'), ('in', 'ongoing', 'fighting')]\n",
            "[('</s>', '</s>', 'Insurgents'), ('</s>', 'Insurgents', 'killed'), ('Insurgents', 'killed', 'in'), ('killed', 'in', 'ongoing'), ('in', 'ongoing', 'fighting')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = 'Good muffins cost $3.88\\nin New York. Please buy me ... two of them.\\n\\nThanks.'\n",
        "trigrams_1 = ngrams(s, n=3, pad_left=True)\n",
        "trigrams_2 = trigrams(s, pad_left=True)\n",
        "\n",
        "print(list(trigrams_1))\n",
        "print(list(trigrams_2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mGKqHq3PZzN",
        "outputId": "119c983c-1029-492b-bad2-0aabb3748147"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(None, None, 'G'), (None, 'G', 'o'), ('G', 'o', 'o'), ('o', 'o', 'd'), ('o', 'd', ' '), ('d', ' ', 'm'), (' ', 'm', 'u'), ('m', 'u', 'f'), ('u', 'f', 'f'), ('f', 'f', 'i'), ('f', 'i', 'n'), ('i', 'n', 's'), ('n', 's', ' '), ('s', ' ', 'c'), (' ', 'c', 'o'), ('c', 'o', 's'), ('o', 's', 't'), ('s', 't', ' '), ('t', ' ', '$'), (' ', '$', '3'), ('$', '3', '.'), ('3', '.', '8'), ('.', '8', '8'), ('8', '8', '\\n'), ('8', '\\n', 'i'), ('\\n', 'i', 'n'), ('i', 'n', ' '), ('n', ' ', 'N'), (' ', 'N', 'e'), ('N', 'e', 'w'), ('e', 'w', ' '), ('w', ' ', 'Y'), (' ', 'Y', 'o'), ('Y', 'o', 'r'), ('o', 'r', 'k'), ('r', 'k', '.'), ('k', '.', ' '), ('.', ' ', 'P'), (' ', 'P', 'l'), ('P', 'l', 'e'), ('l', 'e', 'a'), ('e', 'a', 's'), ('a', 's', 'e'), ('s', 'e', ' '), ('e', ' ', 'b'), (' ', 'b', 'u'), ('b', 'u', 'y'), ('u', 'y', ' '), ('y', ' ', 'm'), (' ', 'm', 'e'), ('m', 'e', ' '), ('e', ' ', '.'), (' ', '.', '.'), ('.', '.', '.'), ('.', '.', ' '), ('.', ' ', 't'), (' ', 't', 'w'), ('t', 'w', 'o'), ('w', 'o', ' '), ('o', ' ', 'o'), (' ', 'o', 'f'), ('o', 'f', ' '), ('f', ' ', 't'), (' ', 't', 'h'), ('t', 'h', 'e'), ('h', 'e', 'm'), ('e', 'm', '.'), ('m', '.', '\\n'), ('.', '\\n', '\\n'), ('\\n', '\\n', 'T'), ('\\n', 'T', 'h'), ('T', 'h', 'a'), ('h', 'a', 'n'), ('a', 'n', 'k'), ('n', 'k', 's'), ('k', 's', '.')]\n",
            "[(None, None, 'G'), (None, 'G', 'o'), ('G', 'o', 'o'), ('o', 'o', 'd'), ('o', 'd', ' '), ('d', ' ', 'm'), (' ', 'm', 'u'), ('m', 'u', 'f'), ('u', 'f', 'f'), ('f', 'f', 'i'), ('f', 'i', 'n'), ('i', 'n', 's'), ('n', 's', ' '), ('s', ' ', 'c'), (' ', 'c', 'o'), ('c', 'o', 's'), ('o', 's', 't'), ('s', 't', ' '), ('t', ' ', '$'), (' ', '$', '3'), ('$', '3', '.'), ('3', '.', '8'), ('.', '8', '8'), ('8', '8', '\\n'), ('8', '\\n', 'i'), ('\\n', 'i', 'n'), ('i', 'n', ' '), ('n', ' ', 'N'), (' ', 'N', 'e'), ('N', 'e', 'w'), ('e', 'w', ' '), ('w', ' ', 'Y'), (' ', 'Y', 'o'), ('Y', 'o', 'r'), ('o', 'r', 'k'), ('r', 'k', '.'), ('k', '.', ' '), ('.', ' ', 'P'), (' ', 'P', 'l'), ('P', 'l', 'e'), ('l', 'e', 'a'), ('e', 'a', 's'), ('a', 's', 'e'), ('s', 'e', ' '), ('e', ' ', 'b'), (' ', 'b', 'u'), ('b', 'u', 'y'), ('u', 'y', ' '), ('y', ' ', 'm'), (' ', 'm', 'e'), ('m', 'e', ' '), ('e', ' ', '.'), (' ', '.', '.'), ('.', '.', '.'), ('.', '.', ' '), ('.', ' ', 't'), (' ', 't', 'w'), ('t', 'w', 'o'), ('w', 'o', ' '), ('o', ' ', 'o'), (' ', 'o', 'f'), ('o', 'f', ' '), ('f', ' ', 't'), (' ', 't', 'h'), ('t', 'h', 'e'), ('h', 'e', 'm'), ('e', 'm', '.'), ('m', '.', '\\n'), ('.', '\\n', '\\n'), ('\\n', '\\n', 'T'), ('\\n', 'T', 'h'), ('T', 'h', 'a'), ('h', 'a', 'n'), ('a', 'n', 'k'), ('n', 'k', 's'), ('k', 's', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "跳元语法"
      ],
      "metadata": {
        "id": "Fife4xUNP5iS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import skipgrams\n",
        "\n",
        "tokens = \"Insurgents killed in ongoing fighting\".split()\n",
        "skipgrams1 = skipgrams(tokens, 3, 2)\n",
        "\n",
        "print(tokens)\n",
        "print(list(skipgrams1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MB1UNWXoPzFT",
        "outputId": "2998585d-ec23-4eaf-d230-bb1951345a0b"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Insurgents', 'killed', 'in', 'ongoing', 'fighting']\n",
            "[('Insurgents', 'killed', 'in'), ('Insurgents', 'killed', 'ongoing'), ('Insurgents', 'killed', 'fighting'), ('Insurgents', 'in', 'ongoing'), ('Insurgents', 'in', 'fighting'), ('Insurgents', 'ongoing', 'fighting'), ('killed', 'in', 'ongoing'), ('killed', 'in', 'fighting'), ('killed', 'ongoing', 'fighting'), ('in', 'ongoing', 'fighting')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 词频与逆向文件频率 tf-idf"
      ],
      "metadata": {
        "id": "uAIkkTmaSBh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 `scikit-learn` 库"
      ],
      "metadata": {
        "id": "sIn4cae9SN7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1.1 统计词频 tf\n",
        "\n",
        "`CountVectorizer` 类"
      ],
      "metadata": {
        "id": "LqHwLAEkTthK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 预料，4 个文档\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]"
      ],
      "metadata": {
        "id": "ozT9bqODSdva"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(analyzer='word')   # stop_words='english' 设置 stop words\n",
        "word_cnt = vectorizer.fit_transform(corpus)     # 统计 词频，返回 sparse matrix 类型\n",
        "word_cnt = word_cnt.toarray()\n",
        "\n",
        "# 输出\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(word_cnt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNFGD-HnTMqF",
        "outputId": "07b93dfd-5c6b-47da-f717-692267cc852c"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
            "[[0 1 1 1 0 0 1 0 1]\n",
            " [0 2 0 1 0 1 1 0 1]\n",
            " [1 0 0 1 1 0 1 1 1]\n",
            " [0 1 1 1 0 0 1 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 字符统计\n",
        "vectorizer1 = CountVectorizer(analyzer='char')\n",
        "word_cnt1 = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# 输出\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(word_cnt1.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JT6tACQQTXmk",
        "outputId": "a9989e1a-f70c-4b16-cf58-81f6ae8c1ff0"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
            "[[0 1 1 1 0 0 1 0 1]\n",
            " [0 2 0 1 0 1 1 0 1]\n",
            " [1 0 0 1 1 0 1 1 1]\n",
            " [0 1 1 1 0 0 1 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# n 元模型\n",
        "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
        "word_cnt2 = vectorizer2.fit_transform(corpus)\n",
        "\n",
        "# 输出\n",
        "print(vectorizer2.get_feature_names_out())\n",
        "print(word_cnt2.toarray())"
      ],
      "metadata": {
        "id": "UE3RK4S_EKCx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62e10a6a-2368-41c6-974d-f23a19cbaaa6"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['and this' 'document is' 'first document' 'is the' 'is this'\n",
            " 'second document' 'the first' 'the second' 'the third' 'third one'\n",
            " 'this document' 'this is' 'this the']\n",
            "[[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
            " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
            " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
            " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "利用 `TfidfTransformer` 类，设置 use_idf=False，统计归一化的词频 tf"
      ],
      "metadata": {
        "id": "kd0qQVS8dLn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 只统计归一化的 tf, 设置 use_idf=False\n",
        "tf_norm = TfidfTransformer(use_idf=False, smooth_idf=True).fit_transform(word_cnt)\n",
        "print(tf_norm.toarray())\n",
        "\n",
        "print(word_cnt / np.linalg.norm(word_cnt, ord=2, axis=1, keepdims=True)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srgyFyIqdSLR",
        "outputId": "12bcf46b-b328-4448-bc06-666bc4886ade"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.4472136  0.4472136  0.4472136  0.         0.\n",
            "  0.4472136  0.         0.4472136 ]\n",
            " [0.         0.70710678 0.         0.35355339 0.         0.35355339\n",
            "  0.35355339 0.         0.35355339]\n",
            " [0.40824829 0.         0.         0.40824829 0.40824829 0.\n",
            "  0.40824829 0.40824829 0.40824829]\n",
            " [0.         0.4472136  0.4472136  0.4472136  0.         0.\n",
            "  0.4472136  0.         0.4472136 ]]\n",
            "[[0.         0.4472136  0.4472136  0.4472136  0.         0.\n",
            "  0.4472136  0.         0.4472136 ]\n",
            " [0.         0.70710678 0.         0.35355339 0.         0.35355339\n",
            "  0.35355339 0.         0.35355339]\n",
            " [0.40824829 0.         0.         0.40824829 0.40824829 0.\n",
            "  0.40824829 0.40824829 0.40824829]\n",
            " [0.         0.4472136  0.4472136  0.4472136  0.         0.\n",
            "  0.4472136  0.         0.4472136 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1.2 统计 idf\n",
        "\n",
        "计算 `tf-idf` 的对象 `TfidfTransformer`，经过拟合后，可以获得 `.idf_` 属性"
      ],
      "metadata": {
        "id": "cZAvFMN_T_WD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "word_cnt = [[3, 0, 1], [2, 0, 0], [3, 0, 0], [4, 0, 0], [3, 2, 0], [3, 0, 2]]\n",
        "# 6 个文档，每个文档有 3 个词\n",
        "\n",
        "transformer = TfidfTransformer(smooth_idf=True)\n",
        "transformer.fit_transform(word_cnt)\n",
        "\n",
        "print(transformer.idf_)\n",
        "print(transformer.n_features_in_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FatPMRj7EKH0",
        "outputId": "599bd165-060e-462e-aefb-09bb38cad0ce"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.         2.25276297 1.84729786]\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1.3 统计 tf-idf"
      ],
      "metadata": {
        "id": "pWmAL0dQUB_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "word_cnt = [[3, 0, 1], [2, 0, 0], [3, 0, 0], [4, 0, 0], [3, 2, 0], [3, 0, 2]]\n",
        "# 6 个文档，每个文档有 3 个词\n",
        "\n",
        "transformer = TfidfTransformer(smooth_idf=True)\n",
        "# 参数: norm='l2', \n",
        "#       use_idf=True, 设所有 idf=1, 即计算归一化的词频\n",
        "#       smooth_idf=True, 是否对 idf 的计算公式平滑\n",
        "\n",
        "# 输入为 统计的词频\n",
        "tfidf = transformer.fit_transform(word_cnt)  # 返回稀疏矩阵\n",
        "print(tfidf.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGCSYaWLUKvZ",
        "outputId": "dd84eea3-fb42-4d34-c8c6-d14833587945"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.85151335 0.         0.52433293]\n",
            " [1.         0.         0.        ]\n",
            " [1.         0.         0.        ]\n",
            " [1.         0.         0.        ]\n",
            " [0.55422893 0.83236428 0.        ]\n",
            " [0.63035731 0.         0.77630514]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 `NLTK` 库"
      ],
      "metadata": {
        "id": "i56zjeiaWyTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.text import TextCollection\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "\n",
        "\n",
        "texts = TextCollection(corpus)\n",
        "\n",
        "for d, doc in enumerate(corpus):\n",
        "    # 分词\n",
        "    tokens = word_tokenize(doc)\n",
        "    # 过滤停词\n",
        "    tokens = [w for w in tokens if w not in ['.', '?']]\n",
        "\n",
        "    print('document {0}'.format(d+1))\n",
        "    # for each word\n",
        "    for w, word in enumerate(tokens):\n",
        "        tf = texts.tf(word, doc)\n",
        "        idf = texts.idf(word)\n",
        "        tf_idf = texts.tf_idf(word, doc)\n",
        "\n",
        "        print('{0:10}'.format(word), end=' ')\n",
        "        print('{0:<8.4f}'.format(tf), end=' ')\n",
        "        print('{0:<8.4f}'.format(idf), end=' ')\n",
        "        print('{0:<8.4f}'.format(tf_idf), end=' ')\n",
        "        print()\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "\n",
        "vectorizer = CountVectorizer(analyzer='word')   # stop_words='english' 设置 stop words\n",
        "word_cnt = vectorizer.fit_transform(corpus)     # 统计 词频\n",
        "\n",
        "transformer = TfidfTransformer(smooth_idf=True)\n",
        "tf_idf = transformer.fit_transform(word_cnt)\n",
        "\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(np.round(tf_idf.toarray(), 4))    # tf-idf\n",
        "print(np.round(transformer.idf_, 4))    # idf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-fsG7aUW-Xa",
        "outputId": "9021219f-7e8e-451e-ee72-55e6aef3eb09"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "document 1\n",
            "This       0.0370   0.6931   0.0257   \n",
            "is         0.0741   0.0000   0.0000   \n",
            "the        0.0370   0.0000   0.0000   \n",
            "first      0.0370   0.6931   0.0257   \n",
            "document   0.0370   0.2877   0.0107   \n",
            "document 2\n",
            "This       0.0270   0.6931   0.0187   \n",
            "document   0.0541   0.2877   0.0156   \n",
            "is         0.0541   0.0000   0.0000   \n",
            "the        0.0270   0.0000   0.0000   \n",
            "second     0.0270   1.3863   0.0375   \n",
            "document   0.0541   0.2877   0.0156   \n",
            "document 3\n",
            "And        0.0385   1.3863   0.0533   \n",
            "this       0.0385   0.6931   0.0267   \n",
            "is         0.0769   0.0000   0.0000   \n",
            "the        0.0385   0.0000   0.0000   \n",
            "third      0.0385   1.3863   0.0533   \n",
            "one        0.0385   1.3863   0.0533   \n",
            "document 4\n",
            "Is         0.0370   1.3863   0.0513   \n",
            "this       0.0370   0.6931   0.0257   \n",
            "the        0.0370   0.0000   0.0000   \n",
            "first      0.0370   0.6931   0.0257   \n",
            "document   0.0370   0.2877   0.0107   \n",
            "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
            "[[0.     0.4698 0.5803 0.3841 0.     0.     0.3841 0.     0.3841]\n",
            " [0.     0.6876 0.     0.2811 0.     0.5386 0.2811 0.     0.2811]\n",
            " [0.5118 0.     0.     0.2671 0.5118 0.     0.2671 0.5118 0.2671]\n",
            " [0.     0.4698 0.5803 0.3841 0.     0.     0.3841 0.     0.3841]]\n",
            "[1.9163 1.2231 1.5108 1.     1.9163 1.9163 1.     1.9163 1.    ]\n"
          ]
        }
      ]
    }
  ]
}